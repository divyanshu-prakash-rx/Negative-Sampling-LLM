{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823a6b6b",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f520316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 30 23:11:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.83                 Driver Version: 576.83         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| 36%   35C    P8              8W /  170W |     125MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2136    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A           15872    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef970bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded for 12GB GPU\n",
      "  Batch size: 8, Max seq: 384\n",
      "  Gradient accumulation: 2\n",
      "  Development Mode: True (100)\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION (12GB GPU OPTIMIZED) ===\n",
    "\n",
    "# API Keys (set if available, else will use Ollama)\n",
    "GEMINI_API_KEY = None  # Set to your API key or leave None for Ollama\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = \"data\"\n",
    "MSMARCO_DIR = f\"{DATA_DIR}/msmarco\"\n",
    "TYDI_DIR = f\"{DATA_DIR}/tydi\"\n",
    "MMARCO_DIR = f\"{DATA_DIR}/mmarco/beir\"\n",
    "\n",
    "# Model directories\n",
    "MODEL_DIR = \"./models\"\n",
    "BASE_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Training configuration for 12GB GPU\n",
    "USE_MIXED_PRECISION = True   # FP16 to save memory, but optional with 12GB\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Can use higher batch size, so less accumulation needed\n",
    "MAX_SEQ_LENGTH = 384         # Safely increase; even 512 should work for most 12GB cards\n",
    "TRAIN_BATCH_SIZE = 8         # You can set 8, test up to 12 if memory allows\n",
    "\n",
    "# Sample sizes for development (set to None for full dataset)\n",
    "DEV_MODE = True              # Set False for full training\n",
    "DEV_SAMPLE_SIZE = 100 if DEV_MODE else None  # Optionally use more for DEV, or set to full\n",
    "\n",
    "# Languages for multilingual training\n",
    "TYDI_LANGUAGES = [\n",
    "    \"arabic\", \"bengali\", \"finnish\"\n",
    "]\n",
    "# , \"indonesian\", \"japanese\",\n",
    "#     \"korean\", \"russian\", \"swahili\", \"telugu\", \"thai\"\n",
    "\n",
    "MMARCO_LANGUAGES = [\n",
    "    \"arabic\", \"chinese\", \"dutch\", \"french\", \"german\",\n",
    "    \"hindi\", \"indonesian\", \"italian\", \"japanese\", \"portuguese\",\n",
    "    \"russian\", \"spanish\", \"vietnamese\"\n",
    "]\n",
    "\n",
    "print(\"âœ“ Configuration loaded for 12GB GPU\")\n",
    "print(f\"  Batch size: {TRAIN_BATCH_SIZE}, Max seq: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Development Mode: {DEV_MODE} ({DEV_SAMPLE_SIZE if DEV_MODE else 'FULL'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a29607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ transformers already installed\n",
      "âœ“ datasets already installed\n",
      "âœ“ pandas already installed\n",
      "âœ“ tqdm already installed\n",
      "âœ“ simpletransformers already installed\n",
      "Installing faiss-cpu...\n",
      "Installing rank-bm25...\n",
      "âœ“ sentence-transformers already installed\n",
      "âœ“ torch already installed\n",
      "\n",
      "âœ“ All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Run once to install required packages\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        \"transformers\",\n",
    "        \"datasets\",\n",
    "        \"pandas\",\n",
    "        \"tqdm\",\n",
    "        \"simpletransformers\",\n",
    "        \"faiss-cpu\",  # Use faiss-cpu for 4GB GPU, or faiss-gpu if sufficient\n",
    "        \"rank-bm25\",\n",
    "        \"sentence-transformers\",\n",
    "        \"torch\",\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"âœ“ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# For Ollama (if not using Gemini API)\n",
    "# Install separately: https://ollama.ai/download\n",
    "# Then: ollama pull llama3.2:3b\n",
    "print(\"\\nâœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497eb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported\n",
      "  PyTorch version: 2.5.1\n",
      "  CUDA available: True\n",
      "  GPU Memory: 12.88 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Libraries imported\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155778a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading MS MARCO Training Data ===\n",
      "âœ“ MS MARCO Train: 100 samples\n",
      "Columns: ['query_text', 'gold_passage', 'hard_negative']\n",
      "\n",
      "Sample:\n",
      "                                          query_text  \\\n",
      "0                         what are the liberal arts?   \n",
      "1  what is the mechanism of action of fibrinolyti...   \n",
      "\n",
      "                                        gold_passage  \\\n",
      "0  liberal arts. 1. the academic course of instru...   \n",
      "1  BailliÃƒÂ¨re's Clinical Haematology. 6 Mechanism...   \n",
      "\n",
      "                                       hard_negative  \n",
      "0  Liberal Education: An approach to college lear...  \n",
      "1  Be able to diagram the coagulation and fibrino...  \n",
      "\n",
      "Statistics:\n",
      "  Avg query length: 34.5 chars\n",
      "  Avg passage length: 362.6 chars\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=== Loading MS MARCO Training Data ===\")\n",
    "\n",
    "# Load training data\n",
    "msmarco_train = pd.read_csv(\n",
    "    f\"{MSMARCO_DIR}/msmarco-train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    nrows=DEV_SAMPLE_SIZE if DEV_MODE else None\n",
    ")\n",
    "\n",
    "# CRITICAL: Rename columns to match BEIR format requirements\n",
    "msmarco_train = msmarco_train.rename(columns={\n",
    "    \"query\": \"query_text\",\n",
    "    \"positive_passage\": \"gold_passage\",\n",
    "    \"negative_passage\": \"hard_negative\"\n",
    "})\n",
    "\n",
    "print(f\"âœ“ MS MARCO Train: {len(msmarco_train):,} samples\")\n",
    "print(\"Columns:\", msmarco_train.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(msmarco_train.head(2))\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"  Avg query length: {msmarco_train['query_text'].str.len().mean():.1f} chars\")\n",
    "print(f\"  Avg passage length: {msmarco_train['gold_passage'].str.len().mean():.1f} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbbc268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Mr. TyDi (BEIR Format) ===\n",
      "  âœ“ arabic: 100\n",
      "  âœ“ bengali: 100\n",
      "  âœ“ finnish: 100\n",
      "\n",
      "âœ“ Total Mr. TyDi: 300 samples\n",
      "Columns: ['query_text', 'gold_passage']\n",
      "\n",
      "Sample:\n",
      "                   query_text  \\\n",
      "0   Ù…Ù† Ù…Ø®ØªØ±Ø¹ Ø­Ø¨ÙˆØ¨ Ù…Ù†Ø¹ Ø§Ù„Ø­Ù…Ù„ ØŸ   \n",
      "1  Ù…ØªÙ‰ ØªØ£Ø³Ø³Øª Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŸ   \n",
      "\n",
      "                                        gold_passage  \n",
      "0  Ø§Ù„Ø¯ÙƒØªÙˆØ± Ø¬Ø±ÙŠØ¬ÙˆØ±ÙŠ Ø¨Ù†ÙƒÙˆØ³ (22 Ø£ØºØ³Ø·Ø³ 1967 9 Ø£Ø¨Ø±ÙŠÙ„ 1...  \n",
      "1  ÙˆØ¹Ù†Ø¯Ù…Ø§ Ø§Ø¬ØªÙ…Ø¹Øª Ù„Ø¬Ù†Ø© ØªØ­Ø¶ÙŠØ±ÙŠØ© Ù…Ù† Ù…Ù…Ø«Ù„ÙŠÙ† Ø¹Ù† ÙƒÙ„ Ù…Ù† ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n=== Loading Mr. TyDi (BEIR Format) ===\")\n",
    "\n",
    "tydi_data = []\n",
    "\n",
    "for lang in TYDI_LANGUAGES:\n",
    "    base_dir = f\"{TYDI_DIR}/{lang}\"\n",
    "    queries_file = os.path.join(base_dir, \"queries.jsonl\")\n",
    "    corpus_file = os.path.join(base_dir, \"corpus.jsonl\")\n",
    "    qrels_file = os.path.join(base_dir, \"qrels/test.tsv\")\n",
    "\n",
    "    # Check required files\n",
    "    if (os.path.exists(queries_file) and \n",
    "        os.path.exists(corpus_file) and \n",
    "        os.path.exists(qrels_file)):\n",
    "        \n",
    "        queries_df = pd.read_json(queries_file, lines=True)\n",
    "        corpus_df = pd.read_json(corpus_file, lines=True)\n",
    "        qrels_df = pd.read_csv(qrels_file, sep='\\t')\n",
    "\n",
    "        # Merge queries and corpus with qrels\n",
    "        merged = qrels_df.merge(\n",
    "            queries_df.rename(columns={\"_id\": \"query-id\"}),\n",
    "            on=\"query-id\"\n",
    "        ).merge(\n",
    "            corpus_df.rename(columns={\"_id\": \"corpus-id\", \"text\": \"gold_passage\"}),  # Changed here\n",
    "            on=\"corpus-id\"\n",
    "        )\n",
    "\n",
    "        # CRITICAL: Rename to BEIR format columns\n",
    "        df = merged[['text', 'gold_passage']].rename(columns={'text': 'query_text'})  # Changed here\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Sample if dev mode\n",
    "        if DEV_SAMPLE_SIZE:\n",
    "            df = df.sample(min(len(df), 100), random_state=42)\n",
    "        \n",
    "        tydi_data.append(df)\n",
    "        print(f\"  âœ“ {lang}: {len(df):,}\")\n",
    "    else:\n",
    "        print(f\"  âš  Missing files for {lang}\")\n",
    "\n",
    "# Combine all languages\n",
    "tydi_combined = pd.concat(tydi_data, ignore_index=True) if tydi_data else pd.DataFrame(columns=[\"query_text\", \"gold_passage\"])\n",
    "\n",
    "print(f\"\\nâœ“ Total Mr. TyDi: {len(tydi_combined):,} samples\")\n",
    "print(\"Columns:\", tydi_combined.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(tydi_combined.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6626292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Datasets ===\n",
      "MS MARCO Train:\n",
      "  Cleaned: 100 â†’ 100 (100.0% retained)\n",
      "\n",
      "Mr. TyDi:\n",
      "  Cleaned: 298 â†’ 298 (100.0% retained)\n",
      "\n",
      "âœ“ Cleaning complete\n",
      "\n",
      "Final counts:\n",
      "  MS MARCO: 100\n",
      "  Mr. TyDi: 298\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove nulls, duplicates, and invalid samples\"\"\"\n",
    "    initial_size = len(df)\n",
    "    \n",
    "    # Remove nulls\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[\n",
    "        (df['query_text'].str.strip() != '') & \n",
    "        (df['gold_passage'].str.strip() != '')\n",
    "    ]\n",
    "    \n",
    "    # Length constraints (for 12GB GPU)\n",
    "    df = df[\n",
    "        (df['query_text'].str.len() >= 10) &\n",
    "        (df['query_text'].str.len() <= 512) &\n",
    "        (df['gold_passage'].str.len() >= 20) &\n",
    "        (df['gold_passage'].str.len() <= 2048)\n",
    "    ]\n",
    "    \n",
    "    # Remove if negative == positive\n",
    "    if 'hard_negative' in df.columns:\n",
    "        df = df[df['hard_negative'] != df['gold_passage']]\n",
    "    \n",
    "    print(f\"  Cleaned: {initial_size:,} â†’ {len(df):,} ({len(df)/initial_size*100:.1f}% retained)\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "print(\"=== Cleaning Datasets ===\")\n",
    "\n",
    "print(\"MS MARCO Train:\")\n",
    "msmarco_train = clean_dataset(msmarco_train)\n",
    "\n",
    "print(\"\\nMr. TyDi:\")\n",
    "tydi_combined = clean_dataset(tydi_combined)\n",
    "\n",
    "print(\"\\nâœ“ Cleaning complete\")\n",
    "print(f\"\\nFinal counts:\")\n",
    "print(f\"  MS MARCO: {len(msmarco_train):,}\")\n",
    "print(f\"  Mr. TyDi: {len(tydi_combined):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "188b26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Training Examples ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 264624.86it/s]\n",
      "Preparing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:00<00:00, 31810.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MS MARCO: 100 examples\n",
      "âœ“ TyDi: 298 examples\n",
      "\n",
      "âœ“ Data saved to data_processed.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare data in format needed for DPR training\n",
    "\n",
    "@dataclass\n",
    "class TrainingExample:\n",
    "    query: str\n",
    "    positive: str\n",
    "    negatives: List[str]  # Will be populated by sampling methods\n",
    "\n",
    "def prepare_training_data(df: pd.DataFrame, has_negatives: bool = True) -> List[TrainingExample]:\n",
    "    \"\"\"Convert DataFrame to training examples\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing\"):\n",
    "        example = TrainingExample(\n",
    "            query=row['query_text'],\n",
    "            positive=row['gold_passage'],\n",
    "            negatives=[row['negative_passage']] if has_negatives and 'negative_passage' in row else []\n",
    "        )\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "print(\"=== Preparing Training Examples ===\")\n",
    "\n",
    "msmarco_train_examples = prepare_training_data(msmarco_train, has_negatives=True)\n",
    "tydi_train_examples = prepare_training_data(tydi_combined, has_negatives=False)\n",
    "\n",
    "print(f\"âœ“ MS MARCO: {len(msmarco_train_examples):,} examples\")\n",
    "print(f\"âœ“ TyDi: {len(tydi_train_examples):,} examples\")\n",
    "\n",
    "# Save to disk for later use\n",
    "import pickle\n",
    "\n",
    "with open('./data_processed.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'msmarco_train': msmarco_train_examples,\n",
    "        'tydi_train': tydi_train_examples\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nâœ“ Data saved to data_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0189345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š MS MARCO (English):\n",
      "  Training samples: 100\n",
      "  Has pre-mined negatives: Yes\n",
      "\n",
      "ðŸ“Š Mr. TyDi (Multilingual):\n",
      "  Total samples: 298\n",
      "  Languages: 3\n",
      "  Has pre-mined negatives: No (will generate)\n",
      "\n",
      "ðŸ“Š Configuration:\n",
      "  Max sequence length: 384\n",
      "  Mixed precision: True\n",
      "  Gradient accumulation: 2\n",
      "\n",
      "âœ… Phase 1 Complete: Data Preparation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final statistics\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“Š MS MARCO (English):\")\n",
    "print(f\"  Training samples: {len(msmarco_train_examples):,}\")\n",
    "print(f\"  Has pre-mined negatives: Yes\")\n",
    "\n",
    "print(\"\\nðŸ“Š Mr. TyDi (Multilingual):\")\n",
    "print(f\"  Total samples: {len(tydi_train_examples):,}\")\n",
    "print(f\"  Languages: {len(TYDI_LANGUAGES)}\")\n",
    "print(f\"  Has pre-mined negatives: No (will generate)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Configuration:\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Mixed precision: {USE_MIXED_PRECISION}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "\n",
    "print(\"\\nâœ… Phase 1 Complete: Data Preparation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa57e21",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e14f35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ rank_bm25 already installed\n",
      "âœ“ Loaded 100 MS MARCO examples\n",
      "âœ“ Loaded 298 TyDi examples\n"
     ]
    }
   ],
   "source": [
    "# Install BM25 for negative sampling\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    print(\"âœ“ rank_bm25 already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing rank_bm25...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25\"])\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    print(\"âœ“ rank_bm25 installed\")\n",
    "\n",
    "# Load processed data\n",
    "import pickle\n",
    "\n",
    "with open('./data_processed.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    msmarco_train_examples = data['msmarco_train']\n",
    "    tydi_train_examples = data['tydi_train']\n",
    "\n",
    "print(f\"âœ“ Loaded {len(msmarco_train_examples):,} MS MARCO examples\")\n",
    "print(f\"âœ“ Loaded {len(tydi_train_examples):,} TyDi examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c62932db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building BM25 Corpus ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting passages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Corpus size: 100 unique passages\n",
      "Building BM25 index...\n",
      "âœ“ BM25 index built with 100 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25-based hard negative mining\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class BM25NegativeSampler:\n",
    "    \"\"\"Mine hard negatives using BM25\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[str]):\n",
    "        print(\"Building BM25 index...\")\n",
    "        # Tokenize corpus\n",
    "        tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.corpus = corpus\n",
    "        print(f\"âœ“ BM25 index built with {len(corpus):,} documents\")\n",
    "    \n",
    "    def get_hard_negatives(self, query: str, positive_passage: str, top_k: int = 100, n_negatives: int = 1) -> List[str]:\n",
    "        \"\"\"Get hard negatives for a query\"\"\"\n",
    "        # Tokenize query\n",
    "        tokenized_query = query.lower().split()\n",
    "        \n",
    "        # Get top-k candidates from BM25\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Filter out positive passage and select negatives\n",
    "        negatives = []\n",
    "        for idx in top_indices:\n",
    "            candidate = self.corpus[idx]\n",
    "            # Skip if it's the positive passage\n",
    "            if candidate != positive_passage and candidate not in negatives:\n",
    "                negatives.append(candidate)\n",
    "            if len(negatives) >= n_negatives:\n",
    "                break\n",
    "        \n",
    "        # If not enough negatives, add random ones\n",
    "        while len(negatives) < n_negatives:\n",
    "            random_idx = np.random.randint(0, len(self.corpus))\n",
    "            candidate = self.corpus[random_idx]\n",
    "            if candidate != positive_passage and candidate not in negatives:\n",
    "                negatives.append(candidate)\n",
    "        \n",
    "        return negatives[:n_negatives]\n",
    "\n",
    "# Build corpus from MS MARCO\n",
    "print(\"\\n=== Building BM25 Corpus ===\")\n",
    "all_passages = set()\n",
    "\n",
    "for example in tqdm(msmarco_train_examples, desc=\"Collecting passages\"):\n",
    "    all_passages.add(example.positive)\n",
    "    all_passages.update(example.negatives)\n",
    "\n",
    "corpus_list = list(all_passages)\n",
    "print(f\"âœ“ Corpus size: {len(corpus_list):,} unique passages\")\n",
    "\n",
    "# Initialize BM25 sampler\n",
    "bm25_sampler = BM25NegativeSampler(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3084337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mining Hard Negatives with BM25 ===\n",
      "\n",
      "Mining for TyDi examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TyDi: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:00<00:00, 7780.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding BM25 negatives to MS MARCO examples (first 100 for demo)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MS MARCO: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 9021.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Hard negative mining complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Mine hard negatives for training examples that don't have them\n",
    "\n",
    "print(\"\\n=== Mining Hard Negatives with BM25 ===\")\n",
    "\n",
    "# For TyDi examples (no pre-existing negatives)\n",
    "print(\"\\nMining for TyDi examples...\")\n",
    "for example in tqdm(tydi_train_examples[:min(len(tydi_train_examples), 500)], desc=\"TyDi\"):\n",
    "    if len(example.negatives) == 0:\n",
    "        hard_negs = bm25_sampler.get_hard_negatives(\n",
    "            example.query, \n",
    "            example.positive, \n",
    "            top_k=100, \n",
    "            n_negatives=1\n",
    "        )\n",
    "        example.negatives = hard_negs\n",
    "\n",
    "# For MS MARCO examples (already have negatives, but we can add more)\n",
    "print(\"\\nAdding BM25 negatives to MS MARCO examples (first 100 for demo)...\")\n",
    "for example in tqdm(msmarco_train_examples[:100], desc=\"MS MARCO\"):\n",
    "    # Add one more hard negative from BM25\n",
    "    bm25_negs = bm25_sampler.get_hard_negatives(\n",
    "        example.query,\n",
    "        example.positive,\n",
    "        top_k=100,\n",
    "        n_negatives=1\n",
    "    )\n",
    "    # Avoid duplicates\n",
    "    for neg in bm25_negs:\n",
    "        if neg not in example.negatives:\n",
    "            example.negatives.append(neg)\n",
    "\n",
    "print(\"\\nâœ“ Hard negative mining complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb38b81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Training DataFrames\n",
      "âœ“ MS MARCO training: 100 triplets\n",
      "Columns: ['query_text', 'gold_passage', 'hard_negative']\n",
      "\n",
      "Sample:\n",
      "                                          query_text  \\\n",
      "0                         what are the liberal arts?   \n",
      "1  what is the mechanism of action of fibrinolyti...   \n",
      "\n",
      "                                        gold_passage  \\\n",
      "0  liberal arts. 1. the academic course of instru...   \n",
      "1  BailliÃƒÂ¨re's Clinical Haematology. 6 Mechanism...   \n",
      "\n",
      "                                       hard_negative  \n",
      "0  Atrophy vs dystrophy. What are atrophy and dys...  \n",
      "1  Labor Day: What it Means. Labor Day, the first...  \n"
     ]
    }
   ],
   "source": [
    "def convert_to_training_format(examples: List, limit: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Convert training examples to DataFrame for SimpleDPR\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for example in (examples[:limit] if limit else examples):\n",
    "        for negative in example.negatives:\n",
    "            data.append({\n",
    "                \"query_text\": example.query,\n",
    "                \"gold_passage\": example.positive,\n",
    "                \"hard_negative\": negative\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"Preparing Training DataFrames\")\n",
    "\n",
    "train_size = 5000 if DEV_MODE else None\n",
    "msmarco_train_df = convert_to_training_format(msmarco_train_examples, limit=train_size)\n",
    "\n",
    "print(f\"âœ“ MS MARCO training: {len(msmarco_train_df):,} triplets\")\n",
    "print(\"Columns:\", msmarco_train_df.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(msmarco_train_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18d824b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuring DPR Model ===\n",
      "âœ“ Model configuration complete\n",
      "  Data format: beir\n",
      "  Hard negatives: True\n",
      "  Evaluation during training: False\n",
      "  Epochs: 5\n",
      "  Batch size: 8\n",
      "  Learning rate: 1e-06\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.retrieval import RetrievalModel, RetrievalArgs\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "print(\"=== Configuring DPR Model ===\")\n",
    "\n",
    "model_args = RetrievalArgs()\n",
    "\n",
    "# CRITICAL: Data format must be 'beir'\n",
    "model_args.data_format = \"beir\"\n",
    "\n",
    "# Data processing\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.use_cached_eval_features = False\n",
    "model_args.use_hf_datasets = True\n",
    "\n",
    "# Model architecture\n",
    "model_args.include_title = False  # MS MARCO doesn't use titles\n",
    "model_args.max_seq_length = 256   # Research paper setting\n",
    "\n",
    "# Training hyperparameters (from research paper)\n",
    "model_args.num_train_epochs = 5   # Use 40 for full training\n",
    "model_args.train_batch_size = 8\n",
    "model_args.learning_rate = 1e-6\n",
    "model_args.warmup_steps = 5000\n",
    "model_args.save_steps = 300000\n",
    "\n",
    "# CRITICAL: Hard negatives enabled for Phase 1\n",
    "model_args.hard_negatives = True\n",
    "\n",
    "# CRITICAL: Evaluation disabled during Phase 1 pretraining\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.save_model_every_epoch = False\n",
    "\n",
    "# Hardware optimization\n",
    "model_args.n_gpu = 1\n",
    "model_args.fp16 = USE_MIXED_PRECISION\n",
    "model_args.dataloader_num_workers = 4\n",
    "\n",
    "# ANCE disabled for Phase 1\n",
    "model_args.ance_training = False\n",
    "\n",
    "# Output directory\n",
    "model_args.output_dir = f\"{MODEL_DIR}/DPR-BM-msmarco\"\n",
    "\n",
    "print(\"âœ“ Model configuration complete\")\n",
    "print(f\"  Data format: {model_args.data_format}\")\n",
    "print(f\"  Hard negatives: {model_args.hard_negatives}\")\n",
    "print(f\"  Evaluation during training: {model_args.evaluate_during_training}\")\n",
    "print(f\"  Epochs: {model_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {model_args.train_batch_size}\")\n",
    "print(f\"  Learning rate: {model_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3cb3892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\torch\\__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "  Allocated: 4.99GB\n",
      "  Reserved: 5.27GB\n",
      "  Free: 7.61GB\n",
      "  Total: 12.88GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 13.5: Aggressive GPU Memory Cleanup\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Clear PyTorch cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clear all variables from previous runs\n",
    "        import sys\n",
    "        for obj in gc.get_objects():\n",
    "            try:\n",
    "                if torch.is_tensor(obj):\n",
    "                    del obj\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Report memory\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(\"âœ“ GPU memory cleared\")\n",
    "        print(f\"  Allocated: {allocated:.2f}GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f}GB\")\n",
    "        print(f\"  Free: {total - reserved:.2f}GB\")\n",
    "        print(f\"  Total: {total:.2f}GB\")\n",
    "\n",
    "# Clear before training\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e396b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 1: TRAINING DPR WITH BM25 NEGATIVES\n",
      "============================================================\n",
      "âœ“ GPU memory cleared\n",
      "  Allocated: 4.99GB\n",
      "  Reserved: 5.27GB\n",
      "  Free: 7.61GB\n",
      "  Total: 12.88GB\n",
      "\n",
      "Training Configuration:\n",
      "  Dataset: MS MARCO\n",
      "  Training samples: 100\n",
      "  Epochs: 5\n",
      "  Batch size: 8\n",
      "  Learning rate: 1e-06\n",
      "  Max sequence length: 256\n",
      "  Hard negatives: True\n",
      "  Data format: beir\n",
      "  Evaluation during training: False\n",
      "  Save model every epoch: False\n",
      "\n",
      "\n",
      "âŒ Training failed with error:\n",
      "   ValueError: Output directory (outputs/) already exists and is not empty. Set args.overwrite_output_dir = True to overcome.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output directory (outputs/) already exists and is not empty. Set args.overwrite_output_dir = True to overcome.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Train for ALL epochs in one call\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# SimpleDPR handles the epoch loop internally\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdpr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsmarco_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Placeholder, not used since evaluation is disabled\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\simpletransformers\\retrieval\\retrieval_model.py:298\u001b[0m, in \u001b[0;36mRetrievalModel.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, additional_eval_passages, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    294\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir)\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(output_dir)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moverwrite_output_dir\n\u001b[0;32m    297\u001b[0m ):\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) already exists and is not empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Set args.overwrite_output_dir = True to overcome.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(output_dir)\n\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mddp_training:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_encoder\u001b[38;5;241m.\u001b[39mto(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Output directory (outputs/) already exists and is not empty. Set args.overwrite_output_dir = True to overcome."
     ]
    }
   ],
   "source": [
    "from multiprocessing import set_start_method\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: TRAINING DPR WITH BM25 NEGATIVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Set multiprocessing method\n",
    "try:\n",
    "    set_start_method(\"spawn\")\n",
    "except RuntimeError:\n",
    "    pass  # Already set\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Dataset: MS MARCO\")\n",
    "print(f\"  Training samples: {len(msmarco_train):,}\")\n",
    "print(f\"  Epochs: {model_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {model_args.train_batch_size}\")\n",
    "print(f\"  Learning rate: {model_args.learning_rate}\")\n",
    "print(f\"  Max sequence length: {model_args.max_seq_length}\")\n",
    "print(f\"  Hard negatives: {model_args.hard_negatives}\")\n",
    "print(f\"  Data format: {model_args.data_format}\")\n",
    "print(f\"  Evaluation during training: {model_args.evaluate_during_training}\")\n",
    "print(f\"  Save model every epoch: {model_args.save_model_every_epoch}\")\n",
    "print()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train for ALL epochs in one call\n",
    "    # SimpleDPR handles the epoch loop internally\n",
    "    dpr_model.train_model(\n",
    "        msmarco_train,\n",
    "        eval_set=\"dev\"  # Placeholder, not used since evaluation is disabled\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PHASE 1 TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"Model saved to: {model_args.output_dir}\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"âœ“ GPU memory cleared\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âŒ OUT OF MEMORY ERROR\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nTry reducing:\")\n",
    "        print(f\"  - train_batch_size (currently: {model_args.train_batch_size})\")\n",
    "        print(f\"  - max_seq_length (currently: {model_args.max_seq_length})\")\n",
    "        print(f\"  - gradient_accumulation_steps (increase to compensate for smaller batch)\")\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training failed with error:\")\n",
    "    print(f\"   {type(e).__name__}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1 COMPLETE - Ready for Phase 2\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "029d0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING BEIR FORMAT DEV/TEST DATASETS\n",
      "============================================================\n",
      "\n",
      "âœ“ Loading cached evaluation dataset...\n",
      "âœ“ Loaded: 7,437 query-passage pairs\n",
      "\n",
      "Evaluation dataset:\n",
      "  Size: 7,437\n",
      "  Columns: ['query_text', 'gold_passage']\n",
      "  Sample query: how many years did william bradford serve as governor of ply...\n",
      "  Sample passage: http://en.wikipedia.org/wiki/William_Bradford_(Plymouth_Colo...\n",
      "\n",
      "============================================================\n",
      "EVALUATION DATASET READY\n",
      "============================================================\n",
      "\n",
      "âœ“ Total evaluation data: 7,437 query-passage pairs\n",
      "âœ“ Test subset (first 50): 50 samples\n",
      "âœ“ Variable 'msmarco_dev' is now available for evaluation\n",
      "âœ“ Models can now be evaluated on actual MS MARCO dev set\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING BEIR FORMAT DEV/TEST DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if already created\n",
    "eval_data_path = f\"{DATA_DIR}/msmarco_dev_beir.tsv\"\n",
    "\n",
    "if os.path.exists(eval_data_path) and os.path.getsize(eval_data_path) > 100:\n",
    "    print(f\"\\nâœ“ Loading cached evaluation dataset...\")\n",
    "    msmarco_dev = pd.read_csv(eval_data_path, sep=\"\\t\")\n",
    "    print(f\"âœ“ Loaded: {len(msmarco_dev):,} query-passage pairs\")\n",
    "else:\n",
    "    print(\"\\nCreating evaluation dataset from MS MARCO files...\")\n",
    "    \n",
    "    # Load queries with string dtype for IDs\n",
    "    print(\"Loading queries...\")\n",
    "    queries_df = pd.read_csv(\n",
    "        f\"{MSMARCO_DIR}/queries.tsv\", \n",
    "        sep=\"\\t\", \n",
    "        names=['query_id', 'title', 'query_text'],\n",
    "        dtype={'query_id': str},\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"âœ“ Queries: {len(queries_df):,}\")\n",
    "    queries_df['query_id'] = queries_df['query_id'].astype(str)\n",
    "    \n",
    "    # Load corpus with string dtype for IDs\n",
    "    print(\"Loading corpus...\")\n",
    "    corpus_df = pd.read_csv(\n",
    "        f\"{MSMARCO_DIR}/corpus.tsv\", \n",
    "        sep=\"\\t\", \n",
    "        names=['corpus_id', 'title', 'passage'],\n",
    "        dtype={'corpus_id': str},\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"âœ“ Corpus: {len(corpus_df):,}\")\n",
    "    corpus_df['corpus_id'] = corpus_df['corpus_id'].astype(str)\n",
    "    \n",
    "    # Load qrels with string dtype for IDs\n",
    "    print(\"Loading qrels...\")\n",
    "    qrels_df = pd.read_csv(\n",
    "        f\"{MSMARCO_DIR}/devs.tsv\", \n",
    "        sep=\"\\t\", \n",
    "        names=['query_id', 'corpus_id', 'score'],\n",
    "        dtype={'query_id': str, 'corpus_id': str}\n",
    "    )\n",
    "    print(f\"âœ“ Qrels (Dev): {len(qrels_df):,}\")\n",
    "    qrels_df['query_id'] = qrels_df['query_id'].astype(str)\n",
    "    qrels_df['corpus_id'] = qrels_df['corpus_id'].astype(str)\n",
    "    \n",
    "    print(\"\\nMerging into BEIR format...\")\n",
    "    \n",
    "    # Merge qrels + queries\n",
    "    print(\"  Merging qrels with queries...\")\n",
    "    msmarco_dev = qrels_df.merge(\n",
    "        queries_df[['query_id', 'query_text']],\n",
    "        on='query_id',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"    After merge: {len(msmarco_dev):,}\")\n",
    "    \n",
    "    # Merge with corpus\n",
    "    print(\"  Merging with corpus...\")\n",
    "    msmarco_dev = msmarco_dev.merge(\n",
    "        corpus_df[['corpus_id', 'passage']].rename(columns={'passage': 'gold_passage'}),\n",
    "        on='corpus_id',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"    After merge: {len(msmarco_dev):,}\")\n",
    "    \n",
    "    # Keep only required columns\n",
    "    msmarco_dev = msmarco_dev[['query_text', 'gold_passage']].drop_duplicates().dropna()\n",
    "    \n",
    "    print(f\"\\nâœ“ Created: {len(msmarco_dev):,} query-passage pairs\")\n",
    "    \n",
    "    if len(msmarco_dev) > 0:\n",
    "        msmarco_dev.to_csv(eval_data_path, sep=\"\\t\", index=False)\n",
    "        print(f\"âœ“ Saved to {eval_data_path}\")\n",
    "    else:\n",
    "        print(\"âŒ Merge failed - no data!\")\n",
    "\n",
    "\n",
    "print(f\"\\nEvaluation dataset:\")\n",
    "print(f\"  Size: {len(msmarco_dev):,}\")\n",
    "\n",
    "if len(msmarco_dev) > 0:\n",
    "    print(f\"  Columns: {msmarco_dev.columns.tolist()}\")\n",
    "    print(f\"  Sample query: {msmarco_dev.iloc[0]['query_text'][:60]}...\")\n",
    "    print(f\"  Sample passage: {msmarco_dev.iloc[0]['gold_passage'][:60]}...\")\n",
    "    \n",
    "    # Use for evaluation\n",
    "    test_data = msmarco_dev.head(50).copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION DATASET READY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nâœ“ Total evaluation data: {len(msmarco_dev):,} query-passage pairs\")\n",
    "    print(f\"âœ“ Test subset (first 50): {len(test_data):,} samples\")\n",
    "    print(f\"âœ“ Variable 'msmarco_dev' is now available for evaluation\")\n",
    "    print(\"âœ“ Models can now be evaluated on actual MS MARCO dev set\")\n",
    "else:\n",
    "    print(\"âŒ No evaluation data - skipping evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7bff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluation Function\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_dpr_model(model, eval_df, model_name, device, top_k=10, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate DPR model on retrieval task\n",
    "    \n",
    "    Args:\n",
    "        model: RetrievalModel instance\n",
    "        eval_df: DataFrame with columns ['query_text', 'gold_passage']\n",
    "        model_name: Name for logging\n",
    "        device: torch device\n",
    "        top_k: Recall cutoff (default 10)\n",
    "        max_samples: Limit samples for speed (None = use all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples:\n",
    "        eval_subset = eval_df.head(max_samples).copy()\n",
    "        print(f\"Using {len(eval_subset)} samples (limited from {len(eval_df)})\")\n",
    "    else:\n",
    "        eval_subset = eval_df.copy()\n",
    "        print(f\"Using all {len(eval_subset)} samples\")\n",
    "    \n",
    "    mrr_scores = []\n",
    "    ndcg_scores = []\n",
    "    recall_1 = []\n",
    "    recall_5 = []\n",
    "    recall_10 = []\n",
    "    \n",
    "    # Get all passages as corpus\n",
    "    all_passages = eval_subset['gold_passage'].tolist()\n",
    "    print(f\"Corpus size: {len(all_passages)} passages\\n\")\n",
    "    \n",
    "    # Evaluate each query\n",
    "    for idx, row in tqdm(eval_subset.iterrows(), total=len(eval_subset), desc=\"Evaluating\"):\n",
    "        query = row['query_text']\n",
    "        gold_passage = row['gold_passage']\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Encode query\n",
    "                query_features = model.query_tokenizer(\n",
    "                    query, \n",
    "                    padding='max_length',\n",
    "                    truncation=True, \n",
    "                    max_length=256, \n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                query_input_ids = query_features['input_ids'].to(device)\n",
    "                query_attention_mask = query_features['attention_mask'].to(device)\n",
    "                \n",
    "                query_emb = model.query_encoder(\n",
    "                    input_ids=query_input_ids,\n",
    "                    attention_mask=query_attention_mask\n",
    "                )[1].cpu().numpy()\n",
    "                \n",
    "                # Score all passages\n",
    "                passage_scores = []\n",
    "                for passage in all_passages:\n",
    "                    passage_features = model.context_tokenizer(\n",
    "                        passage,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=256,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    \n",
    "                    passage_input_ids = passage_features['input_ids'].to(device)\n",
    "                    passage_attention_mask = passage_features['attention_mask'].to(device)\n",
    "                    \n",
    "                    passage_emb = model.context_encoder(\n",
    "                        input_ids=passage_input_ids,\n",
    "                        attention_mask=passage_attention_mask\n",
    "                    )[1].cpu().numpy()\n",
    "                    \n",
    "                    # Cosine similarity\n",
    "                    score = np.dot(query_emb[0], passage_emb[0]) / (\n",
    "                        np.linalg.norm(query_emb[0]) * np.linalg.norm(passage_emb[0]) + 1e-8\n",
    "                    )\n",
    "                    passage_scores.append(score)\n",
    "                \n",
    "                # Rank passages by score\n",
    "                ranked_idx = np.argsort(passage_scores)[::-1]\n",
    "                \n",
    "                # Find rank of gold passage\n",
    "                gold_rank = len(all_passages) + 1\n",
    "                for rank, pidx in enumerate(ranked_idx):\n",
    "                    if all_passages[pidx] == gold_passage:\n",
    "                        gold_rank = rank + 1\n",
    "                        break\n",
    "                \n",
    "                # Compute metrics\n",
    "                if gold_rank <= top_k:\n",
    "                    mrr_scores.append(1.0 / gold_rank)\n",
    "                else:\n",
    "                    mrr_scores.append(0.0)\n",
    "                \n",
    "                if gold_rank <= top_k:\n",
    "                    ndcg_scores.append(1.0 / np.log2(gold_rank + 1))\n",
    "                else:\n",
    "                    ndcg_scores.append(0.0)\n",
    "                \n",
    "                recall_1.append(1.0 if gold_rank <= 1 else 0.0)\n",
    "                recall_5.append(1.0 if gold_rank <= 5 else 0.0)\n",
    "                recall_10.append(1.0 if gold_rank <= 10 else 0.0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        \"MRR@10\": np.mean(mrr_scores) if mrr_scores else 0.0,\n",
    "        \"nDCG@10\": np.mean(ndcg_scores) if ndcg_scores else 0.0,\n",
    "        \"Recall@1\": np.mean(recall_1) if recall_1 else 0.0,\n",
    "        \"Recall@5\": np.mean(recall_5) if recall_5 else 0.0,\n",
    "        \"Recall@10\": np.mean(recall_10) if recall_10 else 0.0,\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da62bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2 END: EVALUATE BM25 BASELINE MODEL\n",
      "============================================================\n",
      "\n",
      "=== Loading BM25 Baseline Model ===\n",
      "\n",
      "Loading model from: ./models/dpr_bm25_baseline_epoch5\n",
      "âœ“ BM25 Baseline Model loaded from ./models/dpr_bm25_baseline_epoch5\n",
      "âœ“ Model moved to device: cuda\n",
      "\n",
      "============================================================\n",
      "EVALUATING BM25 BASELINE ON MS MARCO DEV\n",
      "============================================================\n",
      "\n",
      "Dev set size: 7,437\n",
      "\n",
      "============================================================\n",
      "Evaluating: BM25 Baseline\n",
      "============================================================\n",
      "\n",
      "Using 50 samples (limited from 7437)\n",
      "Corpus size: 50 passages\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2 BASELINE RESULTS\n",
      "============================================================\n",
      "\n",
      "Stage: Phase 2 - BM25 Negative Sampling\n",
      "Model: ./models/dpr_bm25_baseline_epoch5\n",
      "Dataset: MS MARCO Dev\n",
      "Samples evaluated: 50 (from 7,437 total)\n",
      "\n",
      "Metric               Score          \n",
      "-----------------------------------\n",
      "MRR@10               0.2167         \n",
      "nDCG@10              0.2792         \n",
      "Recall@1             0.1200         \n",
      "Recall@5             0.4000         \n",
      "Recall@10            0.4800         \n",
      "\n",
      "ðŸ“Š This is your BASELINE for comparison with:\n",
      "   - Phase 3: LLM-enhanced model\n",
      "   - Phase 4: RAG-enhanced model\n",
      "   - Phase 5: Final multilingual evaluation\n",
      "\n",
      "âœ“ Baseline results saved to ./models/phase2_baseline_results.json\n",
      "\n",
      "âœ“ Baseline metrics stored in 'phase2_baseline' variable for Phase 3 comparison\n",
      "\n",
      "âœ“ GPU memory cleared\n",
      "\n",
      "============================================================\n",
      "âœ… PHASE 2 EVALUATION COMPLETE - BASELINE ESTABLISHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 END: EVALUATE BM25 BASELINE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch\n",
    "from simpletransformers.retrieval import RetrievalModel, RetrievalArgs\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# Load BM25 Baseline Model (Trained in Phase 2)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Loading BM25 Baseline Model ===\\n\")\n",
    "\n",
    "MODEL_TO_EVAL = \"./models/dpr_bm25_baseline_epoch5\"\n",
    "print(f\"Loading model from: {MODEL_TO_EVAL}\")\n",
    "\n",
    "# Configure evaluation args\n",
    "eval_args = RetrievalArgs()\n",
    "eval_args.data_format = \"beir\"\n",
    "eval_args.max_seq_length = 256\n",
    "eval_args.include_title = False\n",
    "eval_args.hard_negatives = False\n",
    "eval_args.fp16 = USE_MIXED_PRECISION\n",
    "eval_args.eval_batch_size = 8\n",
    "\n",
    "# Load YOUR trained BM25 baseline model\n",
    "try:\n",
    "    dpr_model = RetrievalModel(\n",
    "        model_type=\"custom\",\n",
    "        model_name=MODEL_TO_EVAL,\n",
    "        args=eval_args,\n",
    "        use_cuda=torch.cuda.is_available()\n",
    "    )\n",
    "    print(f\"âœ“ BM25 Baseline Model loaded from {MODEL_TO_EVAL}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading model: {e}\")\n",
    "    print(\"Make sure the model was saved during Phase 2 training\")\n",
    "    raise\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dpr_model.query_encoder = dpr_model.query_encoder.to(device)\n",
    "dpr_model.context_encoder = dpr_model.context_encoder.to(device)\n",
    "print(f\"âœ“ Model moved to device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# Run Evaluation using the reusable function\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING BM25 BASELINE ON MS MARCO DEV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'msmarco_dev' in locals() and len(msmarco_dev) > 0:\n",
    "    print(f\"\\nDev set size: {len(msmarco_dev):,}\")\n",
    "    \n",
    "    # Use the function you defined earlier\n",
    "    metrics = evaluate_dpr_model(\n",
    "        dpr_model, \n",
    "        msmarco_dev, \n",
    "        model_name=\"BM25 Baseline\",\n",
    "        device=device,\n",
    "        top_k=10, \n",
    "        max_samples=50  # Evaluate on 50 samples for speed\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2 BASELINE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nStage: Phase 2 - BM25 Negative Sampling\")\n",
    "    print(f\"Model: {MODEL_TO_EVAL}\")\n",
    "    print(f\"Dataset: MS MARCO Dev\")\n",
    "    print(f\"Samples evaluated: 50 (from {len(msmarco_dev):,} total)\")\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Score':<15}\")\n",
    "    print(\"-\" * 35)\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric:<20} {score:<15.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š This is your BASELINE for comparison with:\")\n",
    "    print(\"   - Phase 3: LLM-enhanced model\")\n",
    "    print(\"   - Phase 4: RAG-enhanced model\")\n",
    "    print(\"   - Phase 5: Final multilingual evaluation\")\n",
    "    \n",
    "    # Save baseline results\n",
    "    results_path = f\"{MODEL_DIR}/phase2_baseline_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"\\nâœ“ Baseline results saved to {results_path}\")\n",
    "    \n",
    "    # Store for later comparison\n",
    "    phase2_baseline = metrics.copy()\n",
    "    print(\"\\nâœ“ Baseline metrics stored in 'phase2_baseline' variable for Phase 3 comparison\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  msmarco_dev not loaded\")\n",
    "    print(\"Make sure you ran the 'CREATING BEIR FORMAT DEV/TEST DATASETS' cell first\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nâœ“ GPU memory cleared\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… PHASE 2 EVALUATION COMPLETE - BASELINE ESTABLISHED\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f297fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Checkpoint info saved\n",
      "\n",
      "============================================================\n",
      "âœ… PHASE 2 COMPLETE: Baseline DPR Training\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Model saved at: ./models/dpr_bm25_msmarco_epoch5\n",
      "ðŸ“Š Training samples: 100\n",
      "ðŸ”§ Next: Phase 3 - LLM Integration\n"
     ]
    }
   ],
   "source": [
    "# Save training metadata\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"stage\": \"1_msmarco_baseline\",\n",
    "    \"model_path\": f\"{MODEL_DIR}/dpr_bm25_msmarco_final\",\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"negative_sampling\": \"BM25\",\n",
    "    \"training_samples\": len(msmarco_train_df),\n",
    "    \"epochs\": model_args.num_train_epochs,\n",
    "    \"batch_size_effective\": model_args.train_batch_size * model_args.gradient_accumulation_steps,\n",
    "    \"max_seq_length\": model_args.max_seq_length,\n",
    "    \"fp16\": model_args.fp16,\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(f\"{MODEL_DIR}/checkpoint_stage1.json\", \"w\") as f:\n",
    "    json.dump(checkpoint_info, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Checkpoint info saved\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… PHASE 2 COMPLETE: Baseline DPR Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“ Model saved at: {MODEL_DIR}/dpr_bm25_msmarco_epoch5\")\n",
    "print(f\"ðŸ“Š Training samples: {len(msmarco_train_df):,}\")\n",
    "print(f\"ðŸ”§ Next: Phase 3 - LLM Integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5f719",
   "metadata": {},
   "source": [
    "## Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "141f7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLM Configuration:\n",
      "  Backend: Ollama (Local)\n",
      "  Model: llama3.1:8b-instruct-q4_K_M\n"
     ]
    }
   ],
   "source": [
    "# === LLM CONFIGURATION (SET THIS FIRST) ===\n",
    "\n",
    "# Choose your LLM backend\n",
    "USE_OLLAMA = True  # Set True for local Ollama, False for Gemini API\n",
    "GEMINI_API_KEY = None  # Set if using Gemini\n",
    "\n",
    "# Ollama model (small model for 4GB GPU)\n",
    "OLLAMA_MODEL = \"llama3.1:8b-instruct-q4_K_M\"  # 3B parameters, ~2GB RAM\n",
    "OLLAMA_URL = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "# LLM parameters\n",
    "LLM_TEMPERATURE = 0.1  # Low for consistent classification\n",
    "LLM_MAX_TOKENS = 50    # Short responses only\n",
    "\n",
    "print(\"âœ“ LLM Configuration:\")\n",
    "print(f\"  Backend: {'Ollama (Local)' if USE_OLLAMA else 'Gemini (API)'}\")\n",
    "print(f\"  Model: {OLLAMA_MODEL if USE_OLLAMA else 'gemini-2.0-flash-exp'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4faa6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing LLM dependencies...\n",
      "  âœ“ requests already installed\n",
      "  âœ“ ollama already installed\n",
      "âœ“ All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    \"\"\"Install package if not already installed\"\"\"\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"  âœ“ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"  Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "print(\"Installing LLM dependencies...\")\n",
    "packages = [\"requests\", \"ollama\"] if USE_OLLAMA else [\"google-generativeai\"]\n",
    "\n",
    "for pkg in packages:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb666411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama connection...\n",
      "âœ“ Ollama is running at http://localhost:11434\n",
      "  Available models: 2\n",
      "  âœ“ llama3.1:8b-instruct-q4_K_M is available\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    print(\"Testing Ollama connection...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            print(f\"âœ“ Ollama is running at {OLLAMA_URL}\")\n",
    "            print(f\"  Available models: {len(models)}\")\n",
    "            \n",
    "            # Check if our model is available\n",
    "            model_names = [m.get(\"name\", \"\") for m in models]\n",
    "            if any(OLLAMA_MODEL in name for name in model_names):\n",
    "                print(f\"  âœ“ {OLLAMA_MODEL} is available\")\n",
    "            else:\n",
    "                print(f\"  âš  {OLLAMA_MODEL} not found!\")\n",
    "                print(f\"  Run: ollama pull {OLLAMA_MODEL}\")\n",
    "        else:\n",
    "            print(\"âŒ Ollama not responding\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ollama not running!\")\n",
    "        print(\"Steps to start Ollama:\")\n",
    "        print(\"  1. Download: https://ollama.ai/download\")\n",
    "        print(\"  2. Run: ollama serve\")\n",
    "        print(f\"  3. In another terminal: ollama pull {OLLAMA_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21eecb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM Classifier...\n",
      "âœ“ LLM Classifier ready\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class LLMHardNegativeClassifier:\n",
    "    \"\"\"Classify negatives as HARD or EASY using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, use_ollama=True, model=None, apikey=None):\n",
    "        self.use_ollama = use_ollama\n",
    "        \n",
    "        if use_ollama:\n",
    "            self.model = model or OLLAMA_MODEL\n",
    "            self.url = OLLAMA_URL\n",
    "        else:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=apikey or GEMINI_API_KEY)\n",
    "            self.model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "    \n",
    "    def call_ollama(self, prompt: str) -> str:\n",
    "        \"\"\"Call Ollama API\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.3,\n",
    "                        \"num_predict\": 50  # Increased for scoring\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"response\"].strip()\n",
    "            else:\n",
    "                return \"ERROR\"\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama error: {e}\")\n",
    "            return \"ERROR\"\n",
    "    \n",
    "    def _extract_hardness_score(self, response: str) -> float:\n",
    "        \"\"\"Extract hardness score (0-1) from LLM response\"\"\"\n",
    "        \n",
    "        response_upper = response.upper()\n",
    "        \n",
    "        # Check for explicit HARD/EASY classification\n",
    "        if \"HARD\" in response_upper:\n",
    "            if \"VERY HARD\" in response_upper or \"EXTREMELY HARD\" in response_upper:\n",
    "                return 0.9\n",
    "            elif \"MODERATELY HARD\" in response_upper or \"FAIRLY HARD\" in response_upper:\n",
    "                return 0.7\n",
    "            else:\n",
    "                return 0.75  # Default HARD\n",
    "        \n",
    "        elif \"EASY\" in response_upper:\n",
    "            if \"VERY EASY\" in response_upper or \"EXTREMELY EASY\" in response_upper:\n",
    "                return 0.1\n",
    "            elif \"MODERATELY EASY\" in response_upper or \"FAIRLY EASY\" in response_upper:\n",
    "                return 0.3\n",
    "            else:\n",
    "                return 0.25  # Default EASY\n",
    "        \n",
    "        # Try to extract numeric score\n",
    "        import re\n",
    "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:%|out of 100|/100)', response)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            if '%' in response or 'out of 100' in response:\n",
    "                return score / 100.0\n",
    "            else:\n",
    "                return min(score, 1.0)\n",
    "        \n",
    "        # Default\n",
    "        return 0.5\n",
    "    \n",
    "    def classify_negative(self, query: str, gold_passage: str, negative_passage: str) -> Dict:\n",
    "        \"\"\"Classify negative and return hardness score (0-1)\"\"\"\n",
    "        \n",
    "        # Truncate for context\n",
    "        query = query[:200]\n",
    "        gold_passage = gold_passage[:300]\n",
    "        negative_passage = negative_passage[:300]\n",
    "        \n",
    "        prompt = f\"\"\"Rate how HARD this negative passage is for training (0-100).\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Gold Passage: {gold_passage}\n",
    "\n",
    "Negative Passage: {negative_passage}\n",
    "\n",
    "HARD negatives (70-100):\n",
    "- Topically related to query\n",
    "- Use similar keywords/entities\n",
    "- BUT contain different/incorrect information\n",
    "- Require semantic understanding to distinguish\n",
    "\n",
    "EASY negatives (0-30):\n",
    "- Clearly unrelated to query\n",
    "- Different topic/domain\n",
    "- No semantic confusion\n",
    "\n",
    "Rate this negative as a hardness score from 0-100:\"\"\"\n",
    "        \n",
    "        if self.use_ollama:\n",
    "            response = self.call_ollama(prompt)\n",
    "        else:\n",
    "            response = self.model.generate_content(prompt).text.strip()\n",
    "        \n",
    "        # Extract hardness score (0-1)\n",
    "        hardness_score = self._extract_hardness_score(response)\n",
    "        is_hard = hardness_score > 0.5\n",
    "        \n",
    "        return {\n",
    "            \"is_hard\": is_hard,\n",
    "            \"hardness_score\": hardness_score,  # â† NEW: continuous score\n",
    "            \"classification\": \"HARD\" if is_hard else \"EASY\",\n",
    "            \"response\": response\n",
    "        }\n",
    "    \n",
    "    def classify_batch(self, examples: List[Dict], max_samples: int = None) -> List[Dict]:\n",
    "        \"\"\"Classify a batch of negatives\"\"\"\n",
    "        \n",
    "        samples = examples[:max_samples] if max_samples else examples\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nClassifying {len(samples)} negatives with LLM...\")\n",
    "        \n",
    "        for example in tqdm(samples, desc=\"LLM Classification\"):\n",
    "            result = self.classify_negative(\n",
    "                example['query_text'],\n",
    "                example['gold_passage'],\n",
    "                example['hard_negative']\n",
    "            )\n",
    "            \n",
    "            example_with_classification = {\n",
    "                **example,\n",
    "                'llm_classification': result['classification'],\n",
    "                'is_hard': result['is_hard'],\n",
    "                'hardness_score': result['hardness_score']  # â† NEW\n",
    "            }\n",
    "            results.append(example_with_classification)\n",
    "        \n",
    "        # Statistics\n",
    "        hard_count = sum(1 for r in results if r['is_hard'] == True)\n",
    "        easy_count = sum(1 for r in results if r['is_hard'] == False)\n",
    "        avg_score = np.mean([r['hardness_score'] for r in results])\n",
    "        \n",
    "        print(f\"\\nâœ“ Classification complete!\")\n",
    "        print(f\"  HARD: {hard_count} ({hard_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"  EASY: {easy_count} ({easy_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"  Average hardness: {avg_score:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize\n",
    "print(\"Initializing LLM Classifier...\")\n",
    "llm_classifier = LLMHardNegativeClassifier(\n",
    "    use_ollama=USE_OLLAMA,\n",
    "    model=OLLAMA_MODEL if USE_OLLAMA else None,\n",
    "    apikey=GEMINI_API_KEY\n",
    ")\n",
    "print(\"âœ“ LLM Classifier ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6018ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPREHENSIVE LLM CLASSIFIER TEST\n",
      "============================================================\n",
      "\n",
      "Running comprehensive classifier tests...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 1: EASY - Completely Unrelated\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is photosynthesis\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Photosynthesis is the process by which plants convert light energy into chemical energy in the form ...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  The Great Wall of China is one of the most impressive architectural feats in human history, stretchi...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: EASY\n",
      "  Classified: HARD\n",
      "  Status: âŒ INCORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as **80**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Th...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 2: HARD - Topically Similar but Wrong Answer\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is photosynthesis\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Photosynthesis is the process by which plants convert light energy into chemical energy in the form ...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Chemosynthesis is the process by which certain organisms use chemical energy instead of light energy...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: HARD\n",
      "  Classified: HARD\n",
      "  Status: âœ… CORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as **80**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Th...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 3: EASY - Different Topic\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: how do vaccines work\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Vaccines work by introducing a weakened or inactive form of a pathogen to stimulate the immune syste...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Python is a high-level programming language known for its simple syntax and readability, used in web...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: EASY\n",
      "  Classified: HARD\n",
      "  Status: âŒ INCORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as **80**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Th...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 4: HARD - Similar Keywords but Different Meaning\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: how do vaccines work\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Vaccines work by introducing a weakened or inactive form of a pathogen to stimulate the immune syste...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Antibiotics work by killing bacteria or preventing their growth, interfering with bacterial cell wal...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: HARD\n",
      "  Classified: HARD\n",
      "  Status: âœ… CORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as **80**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Th...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 5: EASY - Sports vs Science\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is diabetes\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Diabetes is a metabolic disorder characterized by high blood sugar levels due to insufficient insuli...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Basketball is a team sport where two teams of five players compete to shoot a ball through the oppos...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: EASY\n",
      "  Classified: HARD\n",
      "  Status: âŒ INCORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as **80**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Th...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 6: HARD - Similar Medical Terms but Different\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is diabetes\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Diabetes is a metabolic disorder characterized by high blood sugar levels due to insufficient insuli...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Hypertension is a condition characterized by elevated blood pressure, which can lead to heart diseas...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: HARD\n",
      "  Classified: HARD\n",
      "  Status: âœ… CORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as 80.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The pa...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 7: EASY - Historical vs Current\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is artificial intelligence\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Artificial intelligence is the field of computer science dedicated to creating intelligent machines ...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  The Roman Empire was one of the largest and most influential empires in history, lasting over 400 ye...\n",
      "\n",
      "ðŸ“Š Result:\n",
      "  Expected: EASY\n",
      "  Classified: HARD\n",
      "  Status: âŒ INCORRECT\n",
      "  LLM Response: I would rate the hardness of this negative passage as a 20.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The ...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Test 8: HARD - Related ML Concepts\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Query: what is artificial intelligence\n",
      "\n",
      "Gold Passage (correct answer):\n",
      "  Artificial intelligence is the field of computer science dedicated to creating intelligent machines ...\n",
      "\n",
      "Negative Passage (candidate):\n",
      "  Machine learning is a subset of artificial intelligence that focuses on enabling computers to learn ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE LLM CLASSIFIER TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create diverse test cases with known difficulty levels\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Test 1: EASY - Completely Unrelated\",\n",
    "        \"query\": \"what is photosynthesis\",\n",
    "        \"gold\": \"Photosynthesis is the process by which plants convert light energy into chemical energy in the form of glucose. It occurs in the chloroplasts of plant cells.\",\n",
    "        \"negative\": \"The Great Wall of China is one of the most impressive architectural feats in human history, stretching over 13,000 miles across northern China.\",\n",
    "        \"expected\": \"EASY\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 2: HARD - Topically Similar but Wrong Answer\",\n",
    "        \"query\": \"what is photosynthesis\",\n",
    "        \"gold\": \"Photosynthesis is the process by which plants convert light energy into chemical energy in the form of glucose.\",\n",
    "        \"negative\": \"Chemosynthesis is the process by which certain organisms use chemical energy instead of light energy to produce organic compounds from carbon dioxide.\",\n",
    "        \"expected\": \"HARD\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 3: EASY - Different Topic\",\n",
    "        \"query\": \"how do vaccines work\",\n",
    "        \"gold\": \"Vaccines work by introducing a weakened or inactive form of a pathogen to stimulate the immune system to produce antibodies.\",\n",
    "        \"negative\": \"Python is a high-level programming language known for its simple syntax and readability, used in web development, data science, and artificial intelligence.\",\n",
    "        \"expected\": \"EASY\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 4: HARD - Similar Keywords but Different Meaning\",\n",
    "        \"query\": \"how do vaccines work\",\n",
    "        \"gold\": \"Vaccines work by introducing a weakened or inactive form of a pathogen to stimulate the immune system.\",\n",
    "        \"negative\": \"Antibiotics work by killing bacteria or preventing their growth, interfering with bacterial cell walls or protein synthesis.\",\n",
    "        \"expected\": \"HARD\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 5: EASY - Sports vs Science\",\n",
    "        \"query\": \"what is diabetes\",\n",
    "        \"gold\": \"Diabetes is a metabolic disorder characterized by high blood sugar levels due to insufficient insulin production or insulin resistance.\",\n",
    "        \"negative\": \"Basketball is a team sport where two teams of five players compete to shoot a ball through the opposing team's elevated hoop.\",\n",
    "        \"expected\": \"EASY\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 6: HARD - Similar Medical Terms but Different\",\n",
    "        \"query\": \"what is diabetes\",\n",
    "        \"gold\": \"Diabetes is a metabolic disorder characterized by high blood sugar levels due to insufficient insulin production or insulin resistance.\",\n",
    "        \"negative\": \"Hypertension is a condition characterized by elevated blood pressure, which can lead to heart disease and stroke if left untreated.\",\n",
    "        \"expected\": \"HARD\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 7: EASY - Historical vs Current\",\n",
    "        \"query\": \"what is artificial intelligence\",\n",
    "        \"gold\": \"Artificial intelligence is the field of computer science dedicated to creating intelligent machines capable of performing tasks that typically require human intelligence.\",\n",
    "        \"negative\": \"The Roman Empire was one of the largest and most influential empires in history, lasting over 400 years and spanning three continents.\",\n",
    "        \"expected\": \"EASY\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Test 8: HARD - Related ML Concepts\",\n",
    "        \"query\": \"what is artificial intelligence\",\n",
    "        \"gold\": \"Artificial intelligence is the field of computer science dedicated to creating intelligent machines capable of performing tasks that typically require human intelligence.\",\n",
    "        \"negative\": \"Machine learning is a subset of artificial intelligence that focuses on enabling computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        \"expected\": \"HARD\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "print(\"\\nRunning comprehensive classifier tests...\\n\")\n",
    "\n",
    "results_summary = {\n",
    "    \"EASY\": {\"correct\": 0, \"incorrect\": 0},\n",
    "    \"HARD\": {\"correct\": 0, \"incorrect\": 0}\n",
    "}\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for test in test_cases:\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"{test['name']}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    \n",
    "    print(f\"\\nQuery: {test['query']}\")\n",
    "    print(f\"\\nGold Passage (correct answer):\")\n",
    "    print(f\"  {test['gold'][:100]}...\")\n",
    "    print(f\"\\nNegative Passage (candidate):\")\n",
    "    print(f\"  {test['negative'][:100]}...\")\n",
    "    \n",
    "    # Classify\n",
    "    result = llm_classifier.classify_negative(\n",
    "        test['query'],\n",
    "        test['gold'],\n",
    "        test['negative']\n",
    "    )\n",
    "    \n",
    "    classification = result['classification']\n",
    "    expected = test['expected']\n",
    "    is_correct = (classification == expected)\n",
    "    \n",
    "    # Display result\n",
    "    print(f\"\\nðŸ“Š Result:\")\n",
    "    print(f\"  Expected: {expected}\")\n",
    "    print(f\"  Classified: {classification}\")\n",
    "    print(f\"  Status: {'âœ… CORRECT' if is_correct else 'âŒ INCORRECT'}\")\n",
    "    print(f\"  LLM Response: {result['response'][:80]}...\")\n",
    "    \n",
    "    # Track statistics\n",
    "    if expected in results_summary:\n",
    "        if is_correct:\n",
    "            results_summary[expected][\"correct\"] += 1\n",
    "        else:\n",
    "            results_summary[expected][\"incorrect\"] += 1\n",
    "    \n",
    "    detailed_results.append({\n",
    "        \"test_name\": test['name'],\n",
    "        \"expected\": expected,\n",
    "        \"classified\": classification,\n",
    "        \"correct\": is_correct\n",
    "    })\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"TEST SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "total_tests = len(test_cases)\n",
    "total_correct = sum(r[\"correct\"] for r in results_summary.values())\n",
    "total_incorrect = sum(r[\"incorrect\"] for r in results_summary.values())\n",
    "accuracy = (total_correct / total_tests) * 100 if total_tests > 0 else 0\n",
    "\n",
    "print(f\"Total Tests: {total_tests}\")\n",
    "print(f\"Correct: {total_correct} ({accuracy:.1f}%)\")\n",
    "print(f\"Incorrect: {total_incorrect} ({100-accuracy:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPer-Category Performance:\")\n",
    "for category, stats in results_summary.items():\n",
    "    total = stats[\"correct\"] + stats[\"incorrect\"]\n",
    "    if total > 0:\n",
    "        cat_accuracy = (stats[\"correct\"] / total) * 100\n",
    "        print(f\"  {category}: {stats['correct']}/{total} ({cat_accuracy:.1f}%)\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "print(f\"\\n\\nDetailed Results Table:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASSIFIER EVALUATION\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if accuracy >= 87.5:  # 7/8 correct\n",
    "    print(\"âœ… EXCELLENT - Classifier is working correctly!\")\n",
    "    print(\"   It can distinguish between EASY and HARD negatives reliably.\")\n",
    "elif accuracy >= 75:  # 6/8 correct\n",
    "    print(\"âœ… GOOD - Classifier is working reasonably well.\")\n",
    "    print(\"   Some edge cases may need adjustment.\")\n",
    "elif accuracy >= 62.5:  # 5/8 correct\n",
    "    print(\"âš ï¸  FAIR - Classifier has room for improvement.\")\n",
    "    print(\"   Consider tuning the LLM temperature or prompt.\")\n",
    "else:\n",
    "    print(\"âŒ POOR - Classifier needs significant improvement.\")\n",
    "    print(\"   Try using a larger LLM model or different prompt.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "240a44ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: CLASSIFY TRAINING DATA WITH LLM\n",
      "============================================================\n",
      "\n",
      "Classifying 100 negatives...\n",
      "\n",
      "Classifying 100 negatives with LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Classification: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:19<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Classification complete!\n",
      "  HARD: 96 (96.0%)\n",
      "  EASY: 4 (4.0%)\n",
      "\n",
      "âœ“ Classification complete!\n",
      "  Total: 100\n",
      "  HARD: 96\n",
      "  EASY: 4\n",
      "\n",
      "Sample classified data:\n",
      "                                          query_text llm_classification\n",
      "0                         what are the liberal arts?               HARD\n",
      "1  what is the mechanism of action of fibrinolyti...               HARD\n",
      "2                          what is normal plat count               HARD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: CLASSIFY TRAINING DATA WITH LLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classify samples\n",
    "LLM_CLASSIFY_SIZE = 100 if DEV_MODE else 1000\n",
    "\n",
    "print(f\"\\nClassifying {LLM_CLASSIFY_SIZE} negatives...\")\n",
    "\n",
    "# Convert to list of dicts - FIX column names\n",
    "train_examples = msmarco_train_df.head(LLM_CLASSIFY_SIZE).to_dict('records')\n",
    "\n",
    "# Classify with LLM\n",
    "llm_classified = llm_classifier.classify_batch(train_examples, max_samples=LLM_CLASSIFY_SIZE)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "llm_classified_df = pd.DataFrame(llm_classified)\n",
    "\n",
    "print(f\"\\nâœ“ Classification complete!\")\n",
    "print(f\"  Total: {len(llm_classified_df):,}\")\n",
    "print(f\"  HARD: {llm_classified_df['is_hard'].sum():,}\")\n",
    "print(f\"  EASY: {(~llm_classified_df['is_hard']).sum():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample classified data:\")\n",
    "print(llm_classified_df[['query_text', 'llm_classification']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: GENERATE LLM HARD NEGATIVES\n",
      "============================================================\n",
      "\n",
      "âœ“ Filtered HARD negatives: 98\n",
      "\n",
      "Generating additional hard negatives with LLM...\n",
      "Generating 200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/200 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   0%|          | 1/200 [00:04<14:41,  4.43s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   1%|          | 2/200 [00:07<12:46,  3.87s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   2%|â–         | 3/200 [00:12<13:24,  4.08s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   2%|â–         | 4/200 [00:16<13:22,  4.09s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   2%|â–Ž         | 5/200 [00:20<13:09,  4.05s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   3%|â–Ž         | 6/200 [00:22<11:21,  3.51s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   4%|â–Ž         | 7/200 [00:26<12:01,  3.74s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   4%|â–         | 8/200 [00:29<10:52,  3.40s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   4%|â–         | 9/200 [00:33<10:54,  3.43s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   5%|â–Œ         | 10/200 [00:36<10:54,  3.44s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   6%|â–Œ         | 11/200 [00:40<11:27,  3.64s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   6%|â–Œ         | 12/200 [00:42<09:29,  3.03s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   6%|â–‹         | 13/200 [00:46<10:14,  3.29s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   7%|â–‹         | 14/200 [00:51<12:18,  3.97s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   8%|â–Š         | 15/200 [00:56<12:37,  4.09s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   8%|â–Š         | 16/200 [00:59<12:03,  3.93s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   8%|â–Š         | 17/200 [01:04<12:25,  4.07s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:   9%|â–‰         | 18/200 [01:07<11:43,  3.87s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  10%|â–‰         | 19/200 [01:11<11:53,  3.94s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  10%|â–ˆ         | 20/200 [01:14<10:56,  3.65s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  10%|â–ˆ         | 21/200 [01:18<11:04,  3.71s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  11%|â–ˆ         | 22/200 [01:21<10:09,  3.43s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  12%|â–ˆâ–        | 23/200 [01:24<10:01,  3.40s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  12%|â–ˆâ–        | 24/200 [01:28<10:07,  3.45s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  12%|â–ˆâ–Ž        | 25/200 [01:32<11:17,  3.87s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  13%|â–ˆâ–Ž        | 26/200 [01:35<09:41,  3.34s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  14%|â–ˆâ–Ž        | 27/200 [01:39<10:25,  3.62s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  14%|â–ˆâ–        | 28/200 [01:42<10:11,  3.55s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  14%|â–ˆâ–        | 29/200 [01:46<10:00,  3.51s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  15%|â–ˆâ–Œ        | 30/200 [01:50<10:45,  3.80s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  16%|â–ˆâ–Œ        | 31/200 [01:55<11:44,  4.17s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  16%|â–ˆâ–Œ        | 32/200 [01:58<10:28,  3.74s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  16%|â–ˆâ–‹        | 33/200 [02:02<10:58,  3.95s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  17%|â–ˆâ–‹        | 34/200 [02:06<10:49,  3.91s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  18%|â–ˆâ–Š        | 35/200 [02:10<10:56,  3.98s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  18%|â–ˆâ–Š        | 36/200 [02:15<11:16,  4.12s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  18%|â–ˆâ–Š        | 37/200 [02:19<11:25,  4.21s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  19%|â–ˆâ–‰        | 38/200 [02:22<10:20,  3.83s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  20%|â–ˆâ–‰        | 39/200 [02:26<10:26,  3.89s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  20%|â–ˆâ–ˆ        | 40/200 [02:30<10:36,  3.98s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  20%|â–ˆâ–ˆ        | 41/200 [02:34<10:31,  3.97s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  21%|â–ˆâ–ˆ        | 42/200 [02:40<11:31,  4.38s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  22%|â–ˆâ–ˆâ–       | 43/200 [02:44<11:18,  4.32s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  22%|â–ˆâ–ˆâ–       | 44/200 [02:48<11:21,  4.37s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [02:52<11:02,  4.28s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [02:56<10:37,  4.14s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [03:01<10:56,  4.29s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  24%|â–ˆâ–ˆâ–       | 48/200 [03:06<11:23,  4.50s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  24%|â–ˆâ–ˆâ–       | 49/200 [03:10<10:50,  4.31s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [03:14<10:57,  4.38s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [03:20<11:50,  4.77s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [03:24<11:11,  4.54s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [03:28<11:06,  4.53s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [03:31<09:23,  3.86s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [03:34<08:48,  3.64s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [03:39<09:37,  4.01s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [03:42<09:07,  3.83s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [03:47<09:31,  4.02s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [03:50<08:50,  3.76s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [03:54<09:03,  3.88s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [03:58<09:22,  4.05s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [04:02<09:21,  4.07s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [04:05<08:35,  3.76s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [04:10<09:18,  4.10s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [04:16<09:57,  4.43s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [04:20<09:53,  4.43s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [04:22<08:20,  3.76s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [04:26<08:19,  3.79s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [04:30<08:05,  3.71s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [04:33<08:03,  3.72s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [04:38<08:20,  3.88s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [04:41<07:58,  3.73s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [04:46<08:31,  4.03s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [04:50<08:30,  4.05s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [04:54<08:49,  4.24s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [04:59<08:42,  4.22s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [05:04<09:07,  4.45s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [05:09<09:24,  4.62s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [05:12<08:24,  4.17s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [05:16<08:20,  4.17s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [05:19<07:48,  3.94s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [05:24<08:05,  4.11s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [05:28<08:14,  4.23s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [05:33<08:32,  4.42s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [05:37<08:13,  4.29s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [05:40<07:26,  3.91s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [05:43<06:35,  3.50s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [05:47<07:11,  3.85s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [05:51<06:58,  3.77s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [05:54<06:36,  3.60s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [05:57<06:21,  3.50s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [06:01<06:17,  3.49s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [06:04<05:46,  3.24s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [06:09<06:41,  3.79s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [06:13<06:44,  3.86s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [06:16<06:33,  3.78s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [06:20<06:12,  3.61s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [06:22<05:35,  3.29s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [06:27<06:27,  3.84s/it]INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [06:31<06:31,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Generated 200 new samples\n",
      "  Failed: 0\n",
      "  Time: 6.5 minutes\n",
      "\n",
      "âœ“ Final dataset size: 298\n",
      "  From classification: 98\n",
      "  From generation: 200\n",
      "\n",
      "âœ“ Saved to data/llm_classified_generated.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: GENERATE LLM HARD NEGATIVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to keep only hard negatives from classification\n",
    "hard_negatives_df = llm_classified_df[llm_classified_df['is_hard'] == True].copy()\n",
    "\n",
    "# Keep only training columns\n",
    "train_columns = ['query_text', 'gold_passage', 'hard_negative']\n",
    "hard_negatives_df = hard_negatives_df[train_columns]\n",
    "\n",
    "print(f\"\\nâœ“ Filtered HARD negatives: {len(hard_negatives_df):,}\")\n",
    "\n",
    "# Generate additional hard negatives to increase dataset\n",
    "print(\"\\nGenerating additional hard negatives with LLM...\")\n",
    "\n",
    "# Initialize generator\n",
    "class LLMHardNegativeGenerator:\n",
    "    \"\"\"Generate hard negatives using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "    \n",
    "    def generate_negatives(self, query: str, positive_passage: str, num_negatives: int = 1) -> List[str]:\n",
    "        \"\"\"Generate hard negatives\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Generate {num_negatives} HARD NEGATIVE passage(s) for this query-passage pair.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Correct Passage: {positive_passage}\n",
    "\n",
    "Requirements:\n",
    "1. Topically related to query\n",
    "2. Similar keywords as correct passage\n",
    "3. BUT does NOT answer the query\n",
    "4. 50-150 words each\n",
    "\n",
    "Output ONLY passages, numbered 1., 2., etc.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=self.model,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"num_predict\": 400\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            text = response['response'].strip()\n",
    "            negatives = self._parse_response(text, num_negatives)\n",
    "            return negatives\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_response(self, text: str, num_negatives: int) -> List[str]:\n",
    "        \"\"\"Parse LLM response\"\"\"\n",
    "        negatives = []\n",
    "        lines = text.split('\\n')\n",
    "        current_negative = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and any(line.startswith(f\"{i}.\") for i in range(1, 6)):\n",
    "                if current_negative:\n",
    "                    negatives.append(' '.join(current_negative))\n",
    "                    current_negative = []\n",
    "                line = line.split('.', 1)[1].strip() if '.' in line else line\n",
    "            \n",
    "            if line:\n",
    "                current_negative.append(line)\n",
    "        \n",
    "        if current_negative:\n",
    "            negatives.append(' '.join(current_negative))\n",
    "        \n",
    "        return negatives[:num_negatives] if negatives else [text[:500]]\n",
    "\n",
    "# Initialize generator\n",
    "llm_generator = LLMHardNegativeGenerator(OLLAMA_MODEL)\n",
    "\n",
    "# Generate for subset to increase data\n",
    "GENERATION_SIZE = 200 if DEV_MODE else 500\n",
    "\n",
    "print(f\"Generating {GENERATION_SIZE} samples...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "failed_count = 0\n",
    "generated_data = []\n",
    "\n",
    "for idx, row in tqdm(msmarco_train_df.head(GENERATION_SIZE).iterrows(), total=GENERATION_SIZE, desc=\"Generating\"):\n",
    "    query = row['query_text']\n",
    "    positive = row['gold_passage']\n",
    "    \n",
    "    # Generate 1-2 hard negatives\n",
    "    hard_negatives = llm_generator.generate_negatives(query, positive, num_negatives=2)\n",
    "    \n",
    "    if hard_negatives:\n",
    "        for neg in hard_negatives:\n",
    "            generated_data.append({\n",
    "                'query_text': query,\n",
    "                'gold_passage': positive,\n",
    "                'hard_negative': neg\n",
    "            })\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "generated_df = pd.DataFrame(generated_data)\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(generated_df):,} new samples\")\n",
    "print(f\"  Failed: {failed_count}\")\n",
    "print(f\"  Time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "\n",
    "# Combine classified hard negatives with newly generated ones\n",
    "final_train_df = pd.concat([hard_negatives_df, generated_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nâœ“ Final dataset size: {len(final_train_df):,}\")\n",
    "print(f\"  From classification: {len(hard_negatives_df):,}\")\n",
    "print(f\"  From generation: {len(generated_df):,}\")\n",
    "\n",
    "# Save\n",
    "final_train_df.to_csv(f\"{DATA_DIR}/llm_classified_generated.tsv\", sep=\"\\t\", index=False)\n",
    "print(f\"\\nâœ“ Saved to {DATA_DIR}/llm_classified_generated.tsv\")\n",
    "pickle_path = f\"{DATA_DIR}/hard_negatives_df.pkl\"\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(final_train_df, f)\n",
    "\n",
    "print(f\"âœ“ Saved pickle: {pickle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760871f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: TRAIN DPR WITH LLM-ENHANCED DATA\n",
      "============================================================\n",
      "âœ“ GPU memory cleared\n",
      "  Allocated: 1.44GB\n",
      "  Reserved: 1.61GB\n",
      "  Free: 11.28GB\n",
      "  Total: 12.88GB\n",
      "\n",
      "Training on 298 LLM-enhanced samples...\n",
      "  Epochs: 3\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-07\n",
      "  Include title: False\n",
      "\n",
      "Loading Phase 1 model...\n",
      "  Found Phase 1 model at: ./models/dpr_bm25_baseline_epoch5\n",
      "âœ“ Loaded Phase 1 checkpoint\n",
      "\n",
      "Training data verification:\n",
      "  Columns: ['query_text', 'gold_passage', 'hard_negative']\n",
      "  Shape: (298, 3)\n",
      "  Sample row:\n",
      "    query_text                              what are the liberal arts?\n",
      "gold_passage     liberal arts. 1. the academic course of instru...\n",
      "hard_negative    Atrophy vs dystrophy. What are atrophy and dys...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:00<00:00, 1492.50 examples/s]\n",
      "INFO:simpletransformers.retrieval.retrieval_model: Training started\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]INFO:simpletransformers.retrieval.retrieval_model:   Starting fine-tuning.\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\simpletransformers\\retrieval\\retrieval_model.py:503: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n",
      "Epoch 1 of 3:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\simpletransformers\\retrieval\\retrieval_model.py:534: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\simpletransformers\\retrieval\\retrieval_model.py:1659: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (max_idxs == torch.tensor(labels)).sum().cpu().detach().numpy().item()\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epochs 0/3. Running Loss:   21.7188 Correct count: 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [01:11<00:00,  1.88s/it]\n",
      "Epochs 1/3. Running Loss:    7.4645 Correct count: 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [01:12<00:00,  1.92s/it]\n",
      "Epochs 2/3. Running Loss:    1.9789 Correct count: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:14<00:00,  2.60it/s]\n",
      "Epoch 3 of 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:39<00:00, 53.06s/it]\n",
      "INFO:simpletransformers.retrieval.retrieval_model:Saving model into ./models/dpr-llm-enhanced\n",
      "INFO:simpletransformers.retrieval.retrieval_model: Training of ./models/dpr_bm25_baseline_epoch5 model complete. Saved to ./models/dpr-llm-enhanced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… PHASE 3 COMPLETE!\n",
      "============================================================\n",
      "Model saved to: ./models/dpr-llm-enhanced\n",
      "Training time: 2.7 minutes\n",
      "Samples processed: 298\n",
      "Avg time per sample: 0.54 seconds\n",
      "âœ“ GPU memory cleared\n",
      "\n",
      "============================================================\n",
      "PHASE 3 SUMMARY\n",
      "============================================================\n",
      "  âœ“ Classified training data with LLM\n",
      "  âœ“ Generated additional hard negatives\n",
      "  âœ“ Trained DPR with LLM-enhanced data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: TRAIN DPR WITH LLM-ENHANCED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Configuration\n",
    "phase3_args = RetrievalArgs()\n",
    "phase3_args.data_format = \"beir\"\n",
    "phase3_args.hard_negatives = True\n",
    "phase3_args.num_train_epochs = 3\n",
    "phase3_args.train_batch_size = 8\n",
    "phase3_args.learning_rate = 5e-7\n",
    "phase3_args.max_seq_length = 256\n",
    "phase3_args.output_dir = f\"{MODEL_DIR}/dpr_llm_enhanced\"\n",
    "phase3_args.fp16 = USE_MIXED_PRECISION\n",
    "phase3_args.evaluate_during_training = False\n",
    "phase3_args.save_model_every_epoch = False\n",
    "phase3_args.overwrite_output_dir = True\n",
    "phase3_args.include_title = False  # CRITICAL: No title column in our data\n",
    "\n",
    "print(f\"\\nTraining on {len(final_train_df):,} LLM-enhanced samples...\")\n",
    "print(f\"  Epochs: {phase3_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {phase3_args.train_batch_size}\")\n",
    "print(f\"  Learning rate: {phase3_args.learning_rate}\")\n",
    "print(f\"  Include title: {phase3_args.include_title}\")\n",
    "\n",
    "try:\n",
    "    # Load pretrained model from Phase 1\n",
    "    print(\"\\nLoading Phase 1 model...\")\n",
    "    \n",
    "    # Find the latest Phase 1 checkpoint\n",
    "    phase1_base_path = f\"{MODEL_DIR}/dpr_bm25_baseline_epoch5\"  # Your latest epoch\n",
    "    \n",
    "    if os.path.exists(phase1_base_path):\n",
    "        print(f\"  Found Phase 1 model at: {phase1_base_path}\")\n",
    "        \n",
    "        # Load with the context encoder path\n",
    "        dpr_model = RetrievalModel(\n",
    "            model_type=\"custom\",\n",
    "            model_name=phase1_base_path,  # Load the full checkpoint\n",
    "            args=phase3_args,\n",
    "            use_cuda=torch.cuda.is_available()\n",
    "        )\n",
    "        print(\"âœ“ Loaded Phase 1 checkpoint\")\n",
    "    else:\n",
    "        print(f\"  âš  Phase 1 model not found at {phase1_base_path}\")\n",
    "        print(\"  Initializing fresh model instead...\")\n",
    "        \n",
    "        dpr_model = RetrievalModel(\n",
    "            model_type=\"custom\",\n",
    "            model_name=None,\n",
    "            context_encoder_name=\"bert-base-multilingual-cased\",\n",
    "            query_encoder_name=\"bert-base-multilingual-cased\",\n",
    "            args=phase3_args,\n",
    "            use_cuda=torch.cuda.is_available()\n",
    "        )\n",
    "        print(\"âœ“ Initialized fresh model\")\n",
    "    \n",
    "    # Verify training data\n",
    "    print(f\"\\nTraining data verification:\")\n",
    "    print(f\"  Columns: {final_train_df.columns.tolist()}\")\n",
    "    print(f\"  Shape: {final_train_df.shape}\")\n",
    "    print(f\"  Sample row:\")\n",
    "    print(f\"    {final_train_df.iloc[0]}\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dpr_model.train_model(final_train_df)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PHASE 3 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model saved to: {phase3_args.output_dir}\")\n",
    "    print(f\"Training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"Samples processed: {len(final_train_df):,}\")\n",
    "    print(f\"Avg time per sample: {(training_time/len(final_train_df)):.2f} seconds\")\n",
    "    \n",
    "    # Clear GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"âœ“ GPU memory cleared\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"  âœ“ Classified training data with LLM\")\n",
    "print(\"  âœ“ Generated additional hard negatives\")\n",
    "print(\"  âœ“ Trained DPR with LLM-enhanced data\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12191e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL EVALUATION ON MS MARCO DEV SET\n",
      "============================================================\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Loading models...\n",
      "âœ“ BM25 Model loaded\n",
      "âœ“ GPU memory cleared\n",
      "  Allocated: 2.86GB\n",
      "  Reserved: 3.01GB\n",
      "  Free: 9.87GB\n",
      "  Total: 12.88GB\n",
      "âœ“ LLM Model loaded\n",
      "Evaluation dataset: 7,437 query-passage pairs\n",
      "\n",
      "\n",
      "============================================================\n",
      "Evaluating: BM25 Baseline\n",
      "============================================================\n",
      "\n",
      "Using 50 samples (limited from 7437)\n",
      "Corpus size: 50 passages\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:38<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "  Allocated: 4.28GB\n",
      "  Reserved: 4.53GB\n",
      "  Free: 8.36GB\n",
      "  Total: 12.88GB\n",
      "\n",
      "============================================================\n",
      "Evaluating: LLM-Enhanced Model\n",
      "============================================================\n",
      "\n",
      "Using 50 samples (limited from 7437)\n",
      "Corpus size: 50 passages\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:40<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS - COMPARISON\n",
      "============================================================\n",
      "\n",
      "   Metric BM25 Baseline LLM-Enhanced Improvement %\n",
      "   MRR@10        0.2167       0.1125       -48.07%\n",
      "  nDCG@10        0.2792       0.1426       -48.93%\n",
      " Recall@1        0.1200       0.0600       -50.00%\n",
      " Recall@5        0.4000       0.1800       -55.00%\n",
      "Recall@10        0.4800       0.2400       -50.00%\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Average Improvement: -50.40%\n",
      "âš ï¸  BM25 model performs better (50.40% better)\n",
      "\n",
      "âœ“ Results saved to ./models/phase3_comparison_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\torch\\__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "  Allocated: 4.28GB\n",
      "  Reserved: 4.53GB\n",
      "  Free: 8.36GB\n",
      "  Total: 12.88GB\n",
      "\n",
      "============================================================\n",
      "âœ… PHASE 3 EVALUATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION ON MS MARCO DEV SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Model paths\n",
    "bm25_model_path = f\"{MODEL_DIR}/dpr_bm25_baseline_epoch5\"\n",
    "llm_model_path = f\"{MODEL_DIR}/dpr_llm_enhanced\"\n",
    "\n",
    "# Load eval args\n",
    "eval_args = RetrievalArgs()\n",
    "eval_args.data_format = \"beir\"\n",
    "eval_args.max_seq_length = 256\n",
    "eval_args.include_title = False\n",
    "eval_args.hard_negatives = False\n",
    "eval_args.fp16 = USE_MIXED_PRECISION\n",
    "\n",
    "print(f\"\\nLoading models...\")\n",
    "\n",
    "# Load models\n",
    "bm25_model = RetrievalModel(\n",
    "    model_type=\"custom\",\n",
    "    model_name=bm25_model_path,\n",
    "    args=eval_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "bm25_model.query_encoder = bm25_model.query_encoder.to(device)\n",
    "bm25_model.context_encoder = bm25_model.context_encoder.to(device)\n",
    "print(f\"âœ“ BM25 Model loaded\")\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "llm_model = RetrievalModel(\n",
    "    model_type=\"custom\",\n",
    "    model_name=llm_model_path,\n",
    "    args=eval_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "llm_model.query_encoder = llm_model.query_encoder.to(device)\n",
    "llm_model.context_encoder = llm_model.context_encoder.to(device)\n",
    "print(f\"âœ“ LLM Model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATE BOTH MODELS \n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    print(f\"Evaluation dataset: {len(msmarco_dev):,} query-passage pairs\\n\")\n",
    "    \n",
    "    # Evaluate BM25 Model\n",
    "    bm25_results = evaluate_dpr_model(\n",
    "        bm25_model, \n",
    "        msmarco_dev, \n",
    "        \"BM25 Baseline\", \n",
    "        device,\n",
    "        max_samples=50  # Evaluate on 50 samples for speed\n",
    "    )\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Evaluate LLM-Enhanced Model\n",
    "    llm_results = evaluate_dpr_model(\n",
    "        llm_model, \n",
    "        msmarco_dev, \n",
    "        \"LLM-Enhanced Model\", \n",
    "        device,\n",
    "        max_samples=50  # Same 50 samples for fair comparison\n",
    "    )\n",
    "    \n",
    "    # ============================================================\n",
    "    # Display Results\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS - COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"Metric\": list(bm25_results.keys()),\n",
    "        \"BM25 Baseline\": [f\"{bm25_results[k]:.4f}\" for k in bm25_results.keys()],\n",
    "        \"LLM-Enhanced\": [f\"{llm_results[k]:.4f}\" for k in llm_results.keys()],\n",
    "        \"Improvement %\": [\n",
    "            f\"{((llm_results[k] - bm25_results[k])/max(bm25_results[k], 0.0001) * 100):.2f}%\"\n",
    "            for k in bm25_results.keys()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + results_df.to_string(index=False))\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    avg_improvement = np.mean([\n",
    "        ((llm_results[k] - bm25_results[k])/max(bm25_results[k], 0.0001) * 100)\n",
    "        for k in bm25_results.keys()\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nAverage Improvement: {avg_improvement:.2f}%\")\n",
    "    \n",
    "    if avg_improvement > 0:\n",
    "        print(f\"âœ… LLM-Enhanced model is BETTER ({avg_improvement:.2f}% improvement)\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  BM25 model performs better ({abs(avg_improvement):.2f}% better)\")\n",
    "    \n",
    "    # Save results\n",
    "    import json\n",
    "    results_summary = {\n",
    "        \"BM25\": bm25_results,\n",
    "        \"LLM-Enhanced\": llm_results,\n",
    "        \"Improvement %\": {\n",
    "            k: ((llm_results[k] - bm25_results[k])/max(bm25_results[k], 0.0001) * 100)\n",
    "            for k in bm25_results.keys()\n",
    "        },\n",
    "        \"Average Improvement %\": avg_improvement\n",
    "    }\n",
    "    \n",
    "    results_path = f\"{MODEL_DIR}/phase3_comparison_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    print(f\"\\nâœ“ Results saved to {results_path}\")\n",
    "    \n",
    "    # Clear GPU\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PHASE 3 EVALUATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641ebf1",
   "metadata": {},
   "source": [
    "## Phase 4 - RAG Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94f16ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FAISS already installed\n",
      "  FAISS version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Install FAISS for dense retrieval\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"âœ“ FAISS already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing FAISS...\")\n",
    "    # Use CPU version for compatibility\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-cpu\"])\n",
    "    import faiss\n",
    "    print(\"âœ“ FAISS installed\")\n",
    "\n",
    "print(f\"  FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5858e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building RAG Retrieval Index from LLM-Processed Negatives\n",
      "============================================================\n",
      "\n",
      "=== Loading LLM-Processed Dataset ===\n",
      "\n",
      "Loading from TSV: data/llm_classified_generated.tsv\n",
      "âœ“ Loaded: 298 samples\n",
      "  Columns: ['query_text', 'gold_passage', 'hard_negative']\n",
      "  Sample query: what are the liberal arts?...\n",
      "\n",
      "=== Building Passage Corpus ===\n",
      "\n",
      "âœ“ Corpus size: 162 unique HARD negatives\n",
      "\n",
      "=== Encoding Passage Corpus ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-multilingual-cased. Creating a new one with MEAN pooling.\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Encoder loaded: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Encoded: 162 passages\n",
      "  Dimension: 768\n",
      "\n",
      "=== Building FAISS Index ===\n",
      "\n",
      "âœ“ Index built: 162 passages\n",
      "\n",
      "=== Saving RAG Index ===\n",
      "\n",
      "âœ“ FAISS index: ./models/rag_corpus_index.faiss\n",
      "âœ“ Corpus: ./models/rag_corpus_passages.pkl\n",
      "âœ“ Metadata: ./models/rag_index_metadata.json\n",
      "\n",
      "============================================================\n",
      "âœ… RAG INDEX READY FOR PHASE 4\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\torch\\__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "  Allocated: 4.99GB\n",
      "  Reserved: 5.31GB\n",
      "  Free: 7.57GB\n",
      "  Total: 12.88GB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Building RAG Retrieval Index from LLM-Processed Negatives\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# LOAD FROM SAVED FILES (No re-processing needed!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Loading LLM-Processed Dataset ===\\n\")\n",
    "\n",
    "# Try pickle first (faster)\n",
    "pickle_path = f\"{DATA_DIR}/hard_negatives_df.pkl\"\n",
    "tsv_path = f\"{DATA_DIR}/llm_classified_generated.tsv\"\n",
    "\n",
    "if os.path.exists(pickle_path):\n",
    "    print(f\"Loading from pickle: {pickle_path}\")\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        hard_negatives_df = pickle.load(f)\n",
    "    print(f\"âœ“ Loaded: {len(hard_negatives_df):,} samples\")\n",
    "\n",
    "elif os.path.exists(tsv_path):\n",
    "    print(f\"Loading from TSV: {tsv_path}\")\n",
    "    hard_negatives_df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "    print(f\"âœ“ Loaded: {len(hard_negatives_df):,} samples\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"LLM-processed data not found!\\n\"\n",
    "        f\"Expected: {pickle_path} or {tsv_path}\\n\"\n",
    "        f\"Please run Phase 3 first and save the data.\"\n",
    "    )\n",
    "\n",
    "print(f\"  Columns: {hard_negatives_df.columns.tolist()}\")\n",
    "print(f\"  Sample query: {hard_negatives_df.iloc[0]['query_text'][:60]}...\")\n",
    "\n",
    "# ============================================================\n",
    "# Build Corpus from LLM-Processed Negatives\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Building Passage Corpus ===\\n\")\n",
    "\n",
    "# Use hard_negative column (already classified as HARD by LLM)\n",
    "corpus_passages = hard_negatives_df['hard_negative'].unique().tolist()\n",
    "print(f\"âœ“ Corpus size: {len(corpus_passages):,} unique HARD negatives\")\n",
    "\n",
    "# ============================================================\n",
    "# Encode Corpus Using Sentence Transformers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Encoding Passage Corpus ===\\n\")\n",
    "\n",
    "rag_encoder = SentenceTransformer(BASE_MODEL)\n",
    "rag_encoder.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rag_encoder = rag_encoder.to(device)\n",
    "print(f\"âœ“ Encoder loaded: {BASE_MODEL}\")\n",
    "\n",
    "# Encode in batches\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(corpus_passages), batch_size), desc=\"Encoding\"):\n",
    "    batch_end = min(i + batch_size, len(corpus_passages))\n",
    "    batch = corpus_passages[i:batch_end]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = rag_encoder.encode(\n",
    "            batch,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "corpus_embeddings = np.vstack(all_embeddings)\n",
    "print(f\"\\nâœ“ Encoded: {corpus_embeddings.shape[0]} passages\")\n",
    "print(f\"  Dimension: {corpus_embeddings.shape[1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# Build and Save FAISS Index\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Building FAISS Index ===\\n\")\n",
    "\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "faiss.normalize_L2(corpus_embeddings)\n",
    "index.add(corpus_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"âœ“ Index built: {index.ntotal:,} passages\")\n",
    "\n",
    "# ============================================================\n",
    "# Save Index and Corpus\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Saving RAG Index ===\\n\")\n",
    "\n",
    "# Save FAISS index\n",
    "index_path = f\"{MODEL_DIR}/rag_corpus_index.faiss\"\n",
    "faiss.write_index(index, index_path)\n",
    "print(f\"âœ“ FAISS index: {index_path}\")\n",
    "\n",
    "# Save corpus passages\n",
    "corpus_path = f\"{MODEL_DIR}/rag_corpus_passages.pkl\"\n",
    "with open(corpus_path, \"wb\") as f:\n",
    "    pickle.dump(corpus_passages, f)\n",
    "print(f\"âœ“ Corpus: {corpus_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"corpus_type\": \"LLM-processed hard negatives\",\n",
    "    \"corpus_size\": len(corpus_passages),\n",
    "    \"embedding_model\": BASE_MODEL,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"source_file\": tsv_path if os.path.exists(tsv_path) else pickle_path,\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = f\"{MODEL_DIR}/rag_index_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Metadata: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… RAG INDEX READY FOR PHASE 4\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be2ca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Cell 27: Initializing RAG Context Retriever\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Initializing RAG Context Retriever\n",
      "============================================================\n",
      "\n",
      "=== Loading FAISS Index ===\n",
      "\n",
      "âœ“ FAISS index loaded: rag_corpus_index.faiss\n",
      "âœ“ Corpus loaded: rag_corpus_passages.pkl\n",
      "âœ“ Loading encoder: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-multilingual-cased. Creating a new one with MEAN pooling.\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\mltorch\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retriever Initialized ===\n",
      "  Index size: 162 passages\n",
      "  Corpus size: 162 passages\n",
      "  Device: cuda\n",
      "  Embedding dim: 768\n",
      "\n",
      "âœ… RAG Retriever initialized successfully!\n",
      "\n",
      "=== Testing Retrieval ===\n",
      "\n",
      "Query: What is machine learning?\n",
      "Retrieved 3 context passages:\n",
      "  1. Score: 0.5318\n",
      "     Text: Formative assessment has been a topic of debate among education scholars for yea...\n",
      "  2. Score: 0.4977\n",
      "     Text: The concept of reinsurance is often misunderstood and has led to numerous miscon...\n",
      "  3. Score: 0.4771\n",
      "     Text: The liberal arts have long been a staple of higher education, but their relevanc...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cell 27: Initializing RAG Context Retriever\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class RAGContextRetriever:\n",
    "    \"\"\"Retrieve context passages for negatives using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, index_path, corpus_path, model_name=None, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"\\n=== Loading FAISS Index ===\\n\")\n",
    "        \n",
    "        # Validate files\n",
    "        if not os.path.exists(index_path):\n",
    "            raise FileNotFoundError(f\"Index not found: {index_path}\")\n",
    "        if not os.path.exists(corpus_path):\n",
    "            raise FileNotFoundError(f\"Corpus not found: {corpus_path}\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        print(f\"âœ“ FAISS index loaded: {os.path.basename(index_path)}\")\n",
    "        \n",
    "        # Load corpus\n",
    "        with open(corpus_path, 'rb') as f:\n",
    "            self.corpus = pickle.load(f)\n",
    "        print(f\"âœ“ Corpus loaded: {os.path.basename(corpus_path)}\")\n",
    "        \n",
    "        # Load encoder\n",
    "        if model_name is None:\n",
    "            model_name = BASE_MODEL\n",
    "        \n",
    "        print(f\"âœ“ Loading encoder: {model_name}\")\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.encoder.eval()\n",
    "        self.encoder = self.encoder.to(self.device)\n",
    "        \n",
    "        print(f\"\\n=== Retriever Initialized ===\")\n",
    "        print(f\"  Index size: {self.index.ntotal:,} passages\")\n",
    "        print(f\"  Corpus size: {len(self.corpus):,} passages\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Embedding dim: 768\")\n",
    "    \n",
    "    def retrieve_context(self, query, top_k=5):\n",
    "        \"\"\"Retrieve top-k context passages for a query\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_emb = self.encoder.encode(\n",
    "                query,\n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        \n",
    "        # Normalize\n",
    "        query_emb_normalized = query_emb / (np.linalg.norm(query_emb) + 1e-8)\n",
    "        query_emb_normalized = query_emb_normalized.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_emb_normalized, top_k)\n",
    "        \n",
    "        contexts = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if idx < len(self.corpus):\n",
    "                contexts.append({\n",
    "                    'passage': self.corpus[idx],\n",
    "                    'score': float(score)\n",
    "                })\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "# ============================================================\n",
    "# Initialize Retriever\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing RAG Context Retriever\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Use CORRECT file names from Cell 26\n",
    "    index_path = f\"{MODEL_DIR}/rag_corpus_index.faiss\"  # â† Correct name\n",
    "    corpus_path = f\"{MODEL_DIR}/rag_corpus_passages.pkl\"  # â† Correct name\n",
    "    \n",
    "    rag_retriever = RAGContextRetriever(\n",
    "        index_path=index_path,\n",
    "        corpus_path=corpus_path,\n",
    "        model_name=BASE_MODEL,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… RAG Retriever initialized successfully!\")\n",
    "    \n",
    "    # Test retrieval\n",
    "    print(\"\\n=== Testing Retrieval ===\\n\")\n",
    "    test_query = \"What is machine learning?\"\n",
    "    test_contexts = rag_retriever.retrieve_context(test_query, top_k=3)\n",
    "    \n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(f\"Retrieved {len(test_contexts)} context passages:\")\n",
    "    for i, ctx in enumerate(test_contexts, 1):\n",
    "        print(f\"  {i}. Score: {ctx['score']:.4f}\")\n",
    "        print(f\"     Text: {ctx['passage'][:80]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "870b7f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Initializing RAG Negative Ranker\n",
      "============================================================\n",
      "âœ“ RAG Negative Ranker initialized with caching\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing RAG Negative Ranker\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class RAGNegativeRanker:\n",
    "    \"\"\"Score negatives using LLM with RAG context\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_retriever, llm_classifier):\n",
    "        self.rag_retriever = rag_retriever\n",
    "        self.llm_classifier = llm_classifier  # Your LLMHardNegativeClassifier from Phase 3\n",
    "        self.score_cache = {}  # Cache LLM scores\n",
    "        \n",
    "        print(\"âœ“ RAG Negative Ranker initialized with caching\")\n",
    "    \n",
    "    def _get_cache_key(self, query, gold, negative):\n",
    "        \"\"\"Create cache key from text\"\"\"\n",
    "        return hash((query[:80], gold[:80], negative[:80]))\n",
    "    \n",
    "    def score_negative_with_context(self, query, gold_passage, negative_passage, top_k_context=3):\n",
    "        \"\"\"\n",
    "        Score a negative using LLM with retrieved context\n",
    "        \n",
    "        Uses your LLMHardNegativeClassifier public method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = self._get_cache_key(query, gold_passage, negative_passage)\n",
    "        if cache_key in self.score_cache:\n",
    "            return self.score_cache[cache_key]\n",
    "        \n",
    "        # Retrieve context using RAG\n",
    "        contexts = self.rag_retriever.retrieve_context(query, top_k=top_k_context)\n",
    "        context_str = \"\\n\".join([f\"- {c['passage'][:150]}\" for c in contexts])\n",
    "        \n",
    "        # Create enhanced prompt with context\n",
    "        enhanced_query = f\"{query}\\n\\nRelevant Context:\\n{context_str}\"\n",
    "        \n",
    "        # Use YOUR public LLMClassifier method\n",
    "        result = self.llm_classifier.classify_negative(\n",
    "            query=enhanced_query,\n",
    "            gold_passage=gold_passage,\n",
    "            negative_passage=negative_passage\n",
    "        )\n",
    "        \n",
    "        # Convert classification to score\n",
    "        is_hard = result.get('is_hard', False)\n",
    "        score = 0.8 if is_hard else 0.2\n",
    "        \n",
    "        # Cache result\n",
    "        self.score_cache[cache_key] = score\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def score_batch(self, negatives_df, top_k_context=3):\n",
    "        \"\"\"Score a batch of negatives\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nScoring {len(negatives_df)} negatives with RAG...\")\n",
    "        \n",
    "        for idx, row in tqdm(negatives_df.iterrows(), total=len(negatives_df), desc=\"RAG Scoring\"):\n",
    "            query = row['query_text']\n",
    "            gold = row['gold_passage']\n",
    "            negative = row['hard_negative']\n",
    "            \n",
    "            try:\n",
    "                score = self.score_negative_with_context(\n",
    "                    query, gold, negative,\n",
    "                    top_k_context=top_k_context\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'query_text': query,\n",
    "                    'gold_passage': gold,\n",
    "                    'hard_negative': negative,\n",
    "                    'rag_score': score\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error scoring: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ“ Scored {len(results)} negatives\")\n",
    "        print(f\"  Cache hit rate: {len(self.score_cache)} cached scores\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize ranker\n",
    "rag_ranker = RAGNegativeRanker(rag_retriever, llm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7ff25aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Applying RAG Ranking\n",
      "============================================================\n",
      "\n",
      "Ranking 50 hard negatives...\n",
      "\n",
      "Scoring 50 negatives with RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG Scoring: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:21<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Scored 50 negatives\n",
      "  Cache hit rate: 50 cached scores\n",
      "\n",
      "=== RAG Score Distribution ===\n",
      "  Mean: 0.8000\n",
      "  Std:  0.0000\n",
      "  Min:  0.8000\n",
      "  Max:  0.8000\n",
      "\n",
      "=== Negative Selection ===\n",
      "  Selection threshold: 0.8000\n",
      "  Keeping quality percentile: 50%\n",
      "âœ“ Selected 50 high-quality negatives\n",
      "  Original: 50\n",
      "  Selected: 50 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Applying RAG Ranking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Limit to subset for speed\n",
    "RAG_SAMPLE_SIZE = 50 if DEV_MODE else 200\n",
    "rag_input = hard_negatives_df.head(RAG_SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"\\nRanking {len(rag_input)} hard negatives...\")\n",
    "\n",
    "# Apply RAG scoring\n",
    "rag_ranked = rag_ranker.score_batch(rag_input, top_k_context=3)\n",
    "\n",
    "# Analyze score distribution\n",
    "print(f\"\\n=== RAG Score Distribution ===\")\n",
    "print(f\"  Mean: {rag_ranked['rag_score'].mean():.4f}\")\n",
    "print(f\"  Std:  {rag_ranked['rag_score'].std():.4f}\")\n",
    "print(f\"  Min:  {rag_ranked['rag_score'].min():.4f}\")\n",
    "print(f\"  Max:  {rag_ranked['rag_score'].max():.4f}\")\n",
    "\n",
    "# Select high-quality negatives\n",
    "QUALITY_PERCENTILE = 0.5  # Keep top 50%\n",
    "selection_threshold = rag_ranked['rag_score'].quantile(1 - QUALITY_PERCENTILE)\n",
    "\n",
    "print(f\"\\n=== Negative Selection ===\")\n",
    "print(f\"  Selection threshold: {selection_threshold:.4f}\")\n",
    "print(f\"  Keeping quality percentile: {QUALITY_PERCENTILE*100:.0f}%\")\n",
    "\n",
    "rag_final = rag_ranked[rag_ranked['rag_score'] >= selection_threshold].copy()\n",
    "\n",
    "print(f\"âœ“ Selected {len(rag_final)} high-quality negatives\")\n",
    "print(f\"  Original: {len(rag_input)}\")\n",
    "print(f\"  Selected: {len(rag_final)} ({len(rag_final)/len(rag_input)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46936de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training DPR with RAG-Selected Negatives\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare training data\n",
    "train_df = rag_final[['query_text', 'gold_passage', 'hard_negative']].copy()\n",
    "train_df.columns = ['query_text', 'gold_passage', 'hard_negative']\n",
    "\n",
    "print(f\"\\n=== Training Data ===\")\n",
    "print(f\"  Samples: {len(train_df)}\")\n",
    "print(f\"  Columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Configure training args\n",
    "phase4_args = RetrievalArgs()\n",
    "phase4_args.data_format = \"beir\"\n",
    "phase4_args.max_seq_length = 256\n",
    "phase4_args.include_title = False\n",
    "phase4_args.hard_negatives = True\n",
    "phase4_args.num_train_epochs = 3\n",
    "phase4_args.train_batch_size = 8\n",
    "phase4_args.learning_rate = 5e-7\n",
    "phase4_args.output_dir = f\"{MODEL_DIR}/dpr_rag_phase4\"\n",
    "phase4_args.fp16 = USE_MIXED_PRECISION\n",
    "phase4_args.evaluate_during_training = False\n",
    "phase4_args.save_model_every_epoch = False\n",
    "phase4_args.overwrite_output_dir = True\n",
    "phase4_args.use_cached_eval_features = False\n",
    "\n",
    "print(f\"\\n=== Training Configuration ===\")\n",
    "print(f\"  Epochs: {phase4_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {phase4_args.train_batch_size}\")\n",
    "print(f\"  Learning rate: {phase4_args.learning_rate}\")\n",
    "print(f\"  Max sequence length: {phase4_args.max_seq_length}\")\n",
    "print(f\"  FP16: {phase4_args.fp16}\")\n",
    "\n",
    "# Load Phase 3 model as starting point\n",
    "print(f\"\\n=== Loading Phase 3 Model ===\")\n",
    "\n",
    "phase3_model_path = f\"{MODEL_DIR}/dpr-llm-enhanced\"\n",
    "\n",
    "try:\n",
    "    dpr_rag_model = RetrievalModel(\n",
    "        model_type=\"custom\",\n",
    "        model_name=phase3_model_path,\n",
    "        args=phase4_args,\n",
    "        use_cuda=torch.cuda.is_available()\n",
    "    )\n",
    "    print(f\"âœ“ Loaded Phase 3 model from {phase3_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Could not load Phase 3 model: {e}\")\n",
    "    print(\"Initializing fresh model from HuggingFace...\")\n",
    "    \n",
    "    dpr_rag_model = RetrievalModel(\n",
    "        model_type=\"dpr\",\n",
    "        model_name=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "        query_encoder_name=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "        args=phase4_args,\n",
    "        use_cuda=torch.cuda.is_available()\n",
    "    )\n",
    "    print(\"âœ“ Initialized fresh DPR model\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\n=== Starting Training ===\\n\")\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    dpr_rag_model.train_model(train_df)\n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(f\"\\nâœ“ Training complete: {training_time:.2f} minutes\")\n",
    "    print(f\"âœ“ Model saved to {phase4_args.output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving Phase 4 Metadata\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metadata = {\n",
    "    \"stage\": \"phase_4_rag_integration\",\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    \"model_path\": phase4_args.output_dir,\n",
    "    \"training_samples\": len(train_df),\n",
    "    \"rag_samples_scored\": len(rag_ranked),\n",
    "    \"rag_samples_selected\": len(rag_final),\n",
    "    \"quality_threshold\": float(selection_threshold),\n",
    "    \"quality_percentile\": QUALITY_PERCENTILE,\n",
    "    \"epochs\": phase4_args.num_train_epochs,\n",
    "    \"batch_size\": phase4_args.train_batch_size,\n",
    "    \"learning_rate\": phase4_args.learning_rate,\n",
    "    \"rag_context_retrieval\": \"FAISS IndexFlatIP\",\n",
    "    \"rag_scorer\": \"LLMClassifier with context\",\n",
    "    \"training_time_minutes\": training_time if 'training_time' in locals() else None\n",
    "}\n",
    "\n",
    "metadata_path = f\"{MODEL_DIR}/phase4_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Metadata saved to {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd659b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 4 Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load RAG model\n",
    "print(\"\\n=== Loading RAG Model ===\")\n",
    "\n",
    "eval_args = RetrievalArgs()\n",
    "eval_args.data_format = \"beir\"\n",
    "eval_args.max_seq_length = 256\n",
    "eval_args.include_title = False\n",
    "eval_args.hard_negatives = False\n",
    "eval_args.eval_batch_size = 8\n",
    "eval_args.fp16 = USE_MIXED_PRECISION\n",
    "\n",
    "rag_eval_model = RetrievalModel(\n",
    "    model_type=\"custom\",\n",
    "    model_name=f\"{MODEL_DIR}/dpr_rag_phase4\",\n",
    "    args=eval_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "rag_eval_model.query_encoder = rag_eval_model.query_encoder.to(device)\n",
    "rag_eval_model.context_encoder = rag_eval_model.context_encoder.to(device)\n",
    "\n",
    "print(\"âœ“ RAG model loaded\")\n",
    "\n",
    "# Evaluate using existing function\n",
    "print(\"\\n=== Evaluating RAG Model ===\")\n",
    "\n",
    "if 'msmarco_dev' in locals() and len(msmarco_dev) > 0:\n",
    "    rag_metrics = evaluate_dpr_model(\n",
    "        rag_eval_model,\n",
    "        msmarco_dev,\n",
    "        model_name=\"RAG-Enhanced Model\",\n",
    "        device=device,\n",
    "        top_k=10,\n",
    "        max_samples=50\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 4 COMPARISON: BM25 vs LLM vs RAG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'phase2_baseline' in locals():\n",
    "        comparison_df = pd.DataFrame({\n",
    "            \"Metric\": list(phase2_baseline.keys()),\n",
    "            \"Phase 2 (BM25)\": [f\"{phase2_baseline[k]:.4f}\" for k in phase2_baseline.keys()],\n",
    "            \"Phase 4 (RAG)\": [f\"{rag_metrics[k]:.4f}\" for k in rag_metrics.keys()],\n",
    "            \"Improvement %\": [\n",
    "                f\"{((rag_metrics[k] - phase2_baseline[k])/max(phase2_baseline[k], 0.0001) * 100):.2f}%\"\n",
    "                for k in phase2_baseline.keys()\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Calculate average improvement\n",
    "        avg_improvement = np.mean([\n",
    "            ((rag_metrics[k] - phase2_baseline[k])/max(phase2_baseline[k], 0.0001) * 100)\n",
    "            for k in phase2_baseline.keys()\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nAverage Improvement: {avg_improvement:.2f}%\")\n",
    "        \n",
    "        if avg_improvement > 0:\n",
    "            print(f\"âœ… RAG model shows {avg_improvement:.2f}% improvement!\")\n",
    "        else:\n",
    "            print(f\"âš  RAG model performs {abs(avg_improvement):.2f}% worse than baseline\")\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… PHASE 4 COMPLETE - RAG INTEGRATION FINISHED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
